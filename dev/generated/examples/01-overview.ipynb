{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Score Matching overview\n",
    "\n",
    "This page introduces the Julia package\n",
    "[`ScoreMatching`](https://github.com/JeffFessler/ScoreMatching.jl)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Packages needed here."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MIRTjim: jim, prompt\n",
    "using Distributions: Distribution, Normal, MixtureModel, logpdf, pdf\n",
    "using Distributions: Gamma, Uniform\n",
    "import Distributions: logpdf, pdf\n",
    "import ForwardDiff\n",
    "using LinearAlgebra: tr, norm\n",
    "using LaTeXStrings\n",
    "using Random: shuffle, seed!; seed!(0)\n",
    "using StatsBase: mean, std\n",
    "using Optim: optimize, BFGS, Fminbox\n",
    "import Optim: minimizer\n",
    "import Flux\n",
    "using Flux: Chain, Dense, Adam\n",
    "import Plots\n",
    "using Plots: Plot, plot, plot!, scatter, histogram, quiver!, default, gui\n",
    "using Plots.PlotMeasures: px\n",
    "using InteractiveUtils: versioninfo\n",
    "default(label=\"\", markerstrokecolor=:auto)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following line is helpful when running this file as a script;\n",
    "this way it will prompt user to hit a key after each figure is displayed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isinteractive() ? jim(:prompt, true) : prompt(:prompt);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "Given $T$\n",
    "[IID](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "training data samples\n",
    "$\\bm{x}_1, …, \\bm{x}_T ∈ \\mathbb{R}^N$,\n",
    "we often want to find the parameters\n",
    "$\\bm{θ}$\n",
    "of a model distribution\n",
    "$p(\\bm{x}; \\bm{θ})$\n",
    "that \"best fit\" the data.\n",
    "\n",
    "Maximum-likelihood estimation\n",
    "is impractical for complicated models\n",
    "where the normalizing constant is intractable.\n",
    "\n",
    "[Hyvärinen 2005](http://jmlr.org/papers/v6/hyvarinen05a.html)\n",
    "proposed an alternative called\n",
    "_score matching_\n",
    "that circumvents\n",
    "needing to find the normalizing constant\n",
    "by using the\n",
    "_score function_\n",
    "of the model distribution,\n",
    "defined as\n",
    "$\n",
    "\\bm{s}(\\bm{x}; \\bm{θ}) =\n",
    "\\nabla_{\\bm{x}} \\log p(\\bm{x}; \\bm{θ}).\n",
    "$\n",
    "\n",
    "## Score functions\n",
    "\n",
    "Before describing score matching methods,\n",
    "we first illustrate\n",
    "what a score function looks like.\n",
    "\n",
    "Consider the (improper) model\n",
    "$\n",
    "p(\\bm{x}; \\bm{θ}) = \\frac{1}{Z(\\bm{θ})} \\mathrm{e}^{-β |x_2 - x_1|^α}\n",
    "$\n",
    "where here there are two parameters\n",
    "$\\bm{θ} = (β, α)$,\n",
    "for $β > 0$ and $α > 1$.\n",
    "The _score function_\n",
    "for this model is\n",
    "$$\n",
    "\\bm{s}(\\bm{x}; \\bm{θ})\n",
    "=\n",
    "\\nabla_{\\bm{x}} \\log p(\\bm{x}; \\bm{θ})\n",
    "=\n",
    "- β \\nabla_{\\bm{x}} |x_2 - x_1|^α\n",
    "= α β \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n",
    "|x_2 - x_1|^{α-1} \\mathrm{sign}(x_2 - x_1).\n",
    "$$\n",
    "\n",
    "This example is related to\n",
    "generalized Gaussian image priors,\n",
    "and,\n",
    "for $α=1$,\n",
    "is related to total variation (TV) regularization.\n",
    "\n",
    "Here is a visualization\n",
    "of this 'TV' pdf\n",
    "and the score functions.\n",
    "\n",
    "The quiver plot\n",
    "shows that the score function\n",
    "describe directions that ascend the prior."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function do_quiver!(p::Plot, x, y, dx, dy; thresh=0.02, scale=0.15)\n",
    "    tmp = d -> maximum(abs, filter(!isnan, d))\n",
    "    dmax = max(tmp(dx), tmp(dy))\n",
    "    ix = 5:11:length(x)\n",
    "    iy = 5:11:length(y)\n",
    "    x, y = x .+ 0*y', 0*x .+ y'\n",
    "    x = x[ix,iy]\n",
    "    y = y[ix,iy]\n",
    "    dx = dx[ix,iy] / dmax * scale\n",
    "    dy = dy[ix,iy] / dmax * scale\n",
    "    good = @. (abs(dx) > thresh) | (abs(dy) > thresh)\n",
    "    x = x[good]\n",
    "    y = y[good]\n",
    "    dx = dx[good]\n",
    "    dy = dy[good]\n",
    "    Plots.arrow(:open, :head, 0.001, 0.001)\n",
    "    return quiver!(p, x, y, quiver=(dx,dy);\n",
    "        aspect_ratio = 1,\n",
    "        title = \"TV score quiver\",\n",
    "        color = :red,\n",
    "    )\n",
    "end;\n",
    "\n",
    "if !@isdefined(ptv)\n",
    "    α = 1.01 # fairly close to TV\n",
    "    β = 1\n",
    "    x1 = range(-1, 1, 101) * 2\n",
    "    x2 = range(-1, 1, 101) * 2\n",
    "    tv_pdf2 = @. exp(-β * abs(x2' - x1)^α) # ignoring partition constant\n",
    "    ptv0 = jim(x1, x2, tv_pdf2; title = \"'TV' pdf\", clim = (0, 1),\n",
    "        color=:cividis, xlabel = L\"x_1\", ylabel = L\"x_2\", prompt=false,\n",
    "    )\n",
    "    tv_score1 = @. β * abs(x2' - x1)^(α-1) * sign(x2' - x1)\n",
    "    ptv1 = jim(x1, x2, tv_score1; title = \"TV score₁\", prompt=false,\n",
    "        color=:cividis, xlabel = L\"x_1\", ylabel = L\"x_2\", clim = (-1,1) .* 1.2,\n",
    "    )\n",
    "    tv_score2 = @. -β * abs(x2' - x1)^(α-1) * sign(x2' - x1)\n",
    "    ptv2 = jim(x1, x2, tv_score2; title = \"TV score₂\", prompt=false,\n",
    "        color=:cividis, xlabel = L\"x_1\", ylabel = L\"x_2\", clim = (-1,1) .* 1.2,\n",
    "    )\n",
    "    ptvq = do_quiver!(deepcopy(ptv0), x1, x2, tv_score1, tv_score2)\n",
    "    ptv = plot(ptv0, ptv1, ptvq, ptv2)\n",
    "    # Plots.savefig(\"score-tv.pdf\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Score matching\n",
    "\n",
    "The idea behind the score matching approach\n",
    "to model fitting is\n",
    "$$\n",
    "\\hat{\\bm{θ}} = \\arg \\min_{\\bm{θ}}\n",
    "J(\\bm{θ})\n",
    ",\\qquad\n",
    "J(\\bm{θ}) =\n",
    "\\frac{1}{T} ∑_{t=1}^T\n",
    "\\| \\bm{s}(\\bm{x}_t; \\bm{θ}) - \\bm{s}(\\bm{x}_t) \\|^2\n",
    "$$\n",
    "where\n",
    "$\n",
    "\\bm{s}(\\bm{x}; \\bm{θ}) =\n",
    "\\nabla_{\\bm{x}} \\log p(\\bm{x}; \\bm{θ})\n",
    "$\n",
    "is the _score function_\n",
    "of the model distribution,\n",
    "and\n",
    "$\n",
    "\\bm{s}(\\bm{x}) =\n",
    "\\nabla_{\\bm{x}} \\log p(\\bm{x})\n",
    "$\n",
    "is the _score function_\n",
    "of the (typically unknown) data distribution.\n",
    "\n",
    "[Vincent, 2011](https://doi.org/10.1162/NECO_a_00142)\n",
    "calls this approach\n",
    "_explicit score matching_ (ESM).\n",
    "\n",
    "\n",
    "## 1D example\n",
    "\n",
    "We begin by illustrating the score function\n",
    "for a simple\n",
    "$\\mathcal{N}(8, 3)$\n",
    "distribution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some convenience methods"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "logpdf(d::Distribution) = x -> logpdf(d, x)\n",
    "pdf(d::Distribution) = x -> pdf(d, x)\n",
    "derivative(f::Function) = x -> ForwardDiff.derivative(f, x)\n",
    "gradient(f::Function) = x -> ForwardDiff.gradient(f, x)\n",
    "# hessian(f::Function) = x -> ForwardDiff.hessian(f, x)\n",
    "score(d::Distribution) = derivative(logpdf(d))\n",
    "score_deriv(d::Distribution) = derivative(score(d)); # scalar x only\n",
    "\n",
    "gauss_μ = 8\n",
    "gauss_σ = 3\n",
    "gauss_disn = Expr(:call, :Normal, gauss_μ, gauss_σ)\n",
    "gauss_dist = eval(gauss_disn)\n",
    "xaxis = (L\"x\", (-1,1).*5gauss_σ .+ gauss_μ, (-3:3)*gauss_σ .+ gauss_μ)\n",
    "left_margin = 20px; bottom_margin = 10px\n",
    "pgp = plot(pdf(gauss_dist); label=\"$gauss_disn pdf\", color = :blue,\n",
    "    left_margin, bottom_margin,\n",
    "    xaxis, yaxis = (L\"p(x)\", (0, 0.15), (0:3)*0.05), size=(600,200),\n",
    ")\n",
    "\n",
    "# Plots.savefig(pgp, \"gauss-pdf.pdf\")\n",
    "prompt()\n",
    "\n",
    "ylabel_score1 = L\"s(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\, \\log \\ p(x)\"\n",
    "pgs = plot(derivative(logpdf(gauss_dist)); color=:red, xaxis, size=(600,200),\n",
    "    label = \"$gauss_disn score function\",\n",
    "    yaxis = (ylabel_score1, (-2,2), -3:3), left_margin, bottom_margin,\n",
    ")\n",
    "\n",
    "# Plots.savefig(pgs, \"gauss-score.pdf\")\n",
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Same plots for a gaussian mixture model (GMM)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mix = MixtureModel(Normal, [(2,1), (8,3), (16,2)], [0.3, 0.4, 0.3])\n",
    "mix = MixtureModel(Normal, [(3,1), (13,3)], [0.4, 0.6])\n",
    "xaxis = (L\"x\", (-4,24), [0, 3, 13, 20])\n",
    "pmp = plot(pdf(mix); label=\"Gaussian mixture pdf\", color = :blue,\n",
    "    left_margin, bottom_margin, xaxis, size=(600,200),\n",
    "    yaxis = (L\"p(x)\", (0, 0.17), (0:3)*0.05),\n",
    ")\n",
    "\n",
    "# Plots.savefig(pmp, \"mix-pdf.pdf\")\n",
    "prompt()\n",
    "\n",
    "pms = plot(derivative(logpdf(mix)); color=:red, xaxis, size=(600,200),\n",
    "    label = \"GMM score function\",\n",
    "    yaxis = (ylabel_score1, (-5,5), -4:2:4), left_margin, bottom_margin,\n",
    ")\n",
    "\n",
    "# Plots.savefig(pms, \"mix-score.pdf\")\n",
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Illustration\n",
    "\n",
    "For didactic purposes,\n",
    "we illustrate explicit score matching (ESM)\n",
    "by fitting samples from a\n",
    "[Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution)\n",
    "to a mixture of gaussians."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate training data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(data)\n",
    "    T = 100\n",
    "    gamma_k = 8 # shape\n",
    "    gamma_θ = 1 # scale\n",
    "    gamma_mode = gamma_k > 1 ? (gamma_k - 1) * gamma_θ : 0\n",
    "    gamma_mean = gamma_k * gamma_θ\n",
    "    gamma_std = sqrt(gamma_k) * gamma_θ\n",
    "    data_disn = Expr(:call, :Gamma, gamma_k, gamma_θ)\n",
    "    data_dis = eval(data_disn)\n",
    "    data_score = derivative(logpdf(data_dis))\n",
    "    data = Float32.(rand(data_dis, T))\n",
    "    xlims = (-1, 25)\n",
    "    xticks = [0, floor(Int, minimum(data)), gamma_mode, gamma_mean, ceil(Int, maximum(data))]\n",
    "    xticks = sort(xticks) # ticks that span the data range\n",
    "\n",
    "    pfd = scatter(data, zeros(T); xlims, xticks, color=:black)\n",
    "    plot!(pfd, pdf(data_dis); label=\"$data_disn pdf\",\n",
    "        color = :black, xlabel = L\"x\", ylabel = L\"p(x)\")\n",
    "\n",
    "    psd = plot(data_score; color=:black,\n",
    "        label = \"$(data_disn.args[1]) score function\",\n",
    "        xaxis=(L\"x\", (1,20), xticks), ylims = (-3, 5), yticks=[0,4],\n",
    "    )\n",
    "    psdn = deepcopy(psd)\n",
    "    tmp = score(Normal(mean(data), std(data)))\n",
    "    plot!(psdn, tmp; label = \"Normal score function\", line=:dash, color=:black)\n",
    "\n",
    "    ph = histogram(data; linecolor=:blue,\n",
    "        xlabel=L\"x\", size=(600,300), yaxis=(\"count\", (0,15), 0:5:15),\n",
    "        bins=-1:0.5:25, xlims, xticks, label=\"data histogram\")\n",
    "    # Plots.savefig(ph, \"gamma-data.pdf\")\n",
    "    plot!(ph, x -> T*0.5 * pdf(data_dis)(x);\n",
    "        color=:black, label=\"$data_disn Distribution\")\n",
    "    # Plots.savefig(ph, \"gamma-fit.pdf\")\n",
    "end\n",
    "\n",
    "\n",
    "if false # plots for a talk\n",
    "    pdt = plot(pdf(data_dis); label=\"$data_disn pdf\", color = :blue,\n",
    "        left_margin, bottom_margin,\n",
    "        xaxis = (L\"x\", xlims, xticks), ylabel = L\"p(x)\", size=(600,200))\n",
    "    pst = deepcopy(psd)\n",
    "    tmp = score(Normal(gamma_mean, gamma_std))\n",
    "    plot!(pst, tmp; label = \"Normal score function\", size=(600,200), xlims,\n",
    "        line=:dash, color=:magenta, left_margin, bottom_margin,\n",
    "        ylabel=ylabel_score1)\n",
    "\n",
    "    # Plots.savefig(pdt, \"gamma-pdf.pdf\")\n",
    "    # Plots.savefig(pst, \"gamma-score.pdf\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform unconstrained minimization\n",
    "of a $D$-component mixture,\n",
    "the following mapping from $\\mathbb{R}^{D-1}$\n",
    "to the $D$-dimensional simplex is helpful.\n",
    "It is the inverse of the\n",
    "[additive logratio transform](https://en.wikipedia.org/wiki/Compositional_data#Additive_logratio_transform).\n",
    "It is related to the\n",
    "[softmax function](https://en.wikipedia.org/wiki/Softmax_function)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function map_r_s(y::AbstractVector; scale::Real = 1.0)\n",
    "    y = scale * [y; 0]\n",
    "    y .-= maximum(y) # for numerical stability\n",
    "    p = exp.(y)\n",
    "    return p / sum(p)\n",
    "end\n",
    "map_r_s(y::Real...) = map_r_s([y...])\n",
    "\n",
    "y1 = range(-1,1,101) * 9\n",
    "y2 = range(-1,1,101) * 9\n",
    "tmp = map_r_s.(y1, y2')\n",
    "pj = jim(y1, y2, tmp; title=\"Simplex parameterization\", nrow=1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define model distribution"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nmix = 3 # how many gaussians in the mixture model\n",
    "function model(θ ;\n",
    "    σmin::Real = 1,\n",
    "    σmax::Real = 19,\n",
    ")\n",
    "    mu = θ[1:nmix]\n",
    "    sig = θ[nmix .+ (1:nmix)]\n",
    "    any(<(σmin), sig) && throw(\"too small σ\")\n",
    "    any(>(σmax), sig) && throw(\"too big σ $sig\")\n",
    "    # sig = σmin .+ exp.(sig) # ensure σ > 0\n",
    "    # sig = @. σmin + (σmax - σmin) * (tanh(sig/2) + 1) / 2 # \"constraints\"\n",
    "    p = map_r_s(θ[2nmix .+ (1:(nmix-1))])\n",
    "    tmp = [(μ,σ) for (μ,σ) in zip(mu, sig)]\n",
    "    mix = MixtureModel(Normal, tmp, p)\n",
    "    return mix\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define explicit score-matching cost function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function cost_esm2(x::AbstractVector{<:Real}, θ)\n",
    "    model_score = score(model(θ))\n",
    "    return (0.5/T) * sum(abs2, model_score.(x) - data_score.(x))\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Minimize this explicit score-matching cost function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "β = 0e-4 # optional small regularizer to ensure coercive\n",
    "cost_esm1 = (θ) -> cost_esm2(data, θ) + β * 0.5 * norm(θ)^2;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initial crude guess of mixture model parameters"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "θ0 = Float64[5, 7, 9, 1.5, 1.5, 1.5, 0, 0]; # Gamma"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot data pdf and initial model pdf"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pf = deepcopy(pfd)\n",
    "plot!(pf, pdf(model(θ0)), label = \"Initial Gaussian mixture\", color=:blue)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check descent and non-convexity"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if false\n",
    "    tmp = gradient(cost_esm1)(θ0)\n",
    "    a = range(0, 9, 101)\n",
    "    h = a -> cost_esm1(θ0 - a * tmp)\n",
    "    plot(a, log.(h.(a)))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explicit score matching (ESM) (impractical)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(θesm)\n",
    "    lower = [fill(0, nmix); fill(1.0, nmix); fill(-Inf, nmix-1)]\n",
    "    upper = [fill(Inf, nmix); fill(Inf, nmix); fill(Inf, nmix-1)]\n",
    "    opt_esm = optimize(cost_esm1, lower, upper, θ0, Fminbox(BFGS());\n",
    "        autodiff = :forward)\n",
    "    # opt_esm = optimize(cost_esm1, θ0, BFGS(); autodiff = :forward) # unconstrained\n",
    "    θesm = minimizer(opt_esm)\n",
    "end;\n",
    "\n",
    "plot!(pf, pdf(model(θesm)), label = \"ESM Gaussian mixture\", color=:green)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the data score and model score functions\n",
    "to see how well they match.\n",
    "The largest mismatch is in the tails of the distribution\n",
    "where there are few (if any) data points."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ps = deepcopy(psd)\n",
    "plot!(ps, score(model(θesm)); label = \"ESM score function\", color=:green)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Maximum-likelihood estimation\n",
    "\n",
    "This toy example is simple enough\n",
    "that we can apply ML estimation to it directly.\n",
    "In fact, ML estimation is a seemingly more practical optimization problem\n",
    "than score matching in this case.\n",
    "\n",
    "As expected,\n",
    "ML estimation leads to a lower negative log-likelihood."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "negloglike(θ) = (-1/T) * sum(logpdf(model(θ)), data)\n",
    "opt_ml = optimize(negloglike, lower, upper, θ0, Fminbox(BFGS()); autodiff = :forward)\n",
    "θml = minimizer(opt_ml)\n",
    "negloglike.([θml, θesm, θ0])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Curiously,\n",
    "ML estimation here leads to much worse fits to the pdf\n",
    "than score matching,\n",
    "even though we initialized the ML optimizer\n",
    "with the score-matching parameters.\n",
    "Perhaps the landscape of the log-likelihood\n",
    "is less well-behaved\n",
    "than that of the SM cost."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot!(pf, pdf(model(θml)), label = \"ML Gaussian mixture\", color=:magenta)\n",
    "plot!(ps, score(model(θml)), label = \"ML score function\", color=:magenta)\n",
    "plot(pf, ps)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implicit score matching (ISM) (more practical)\n",
    "\n",
    "The above ESM fitting process\n",
    "used `score(data_dis)`,\n",
    "the score-function of the data distribution,\n",
    "which is unknown in practical situations.\n",
    "\n",
    "[Hyvärinen 2005](http://jmlr.org/papers/v6/hyvarinen05a.html)\n",
    "derived the following more practical cost function\n",
    "that is independent of the unknown data score function:\n",
    "$$\n",
    "J_{\\mathrm{ISM}}(\\bm{θ}) =\n",
    "\\frac{1}{T} ∑_{t=1}^T\n",
    "∑_{i=1}^N ∂_i s_i(\\bm{x}_t; \\bm{θ})\n",
    " + \\frac{1}{2} | s_i(\\bm{x}_t; \\bm{θ}) |^2,\n",
    "$$\n",
    "ignoring a constant that is independent of $θ,$\n",
    "where\n",
    "$$\n",
    "∂_i s_i(\\bm{x}; \\bm{θ})\n",
    "=\n",
    "\\frac{∂}{∂ x_i} s_i(\\bm{x}; \\bm{θ})\n",
    "=\n",
    "\\frac{∂^2}{∂ x_i^2} \\log p(\\bm{x}; \\bm{θ}).\n",
    "$$\n",
    "\n",
    "(For large models\n",
    "this version is still a bit impractical\n",
    "because it depends on the diagonal\n",
    "elements of the Hessian\n",
    "of the log prior.\n",
    "Subsequent pages deal with that issue.)\n",
    "\n",
    "[Vincent, 2011](https://doi.org/10.1162/NECO_a_00142)\n",
    "calls this approach\n",
    "_implicit score matching_ (ISM)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implicit score-matching cost function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function cost_ism2(x::AbstractVector{<:Real}, θ)\n",
    "    tmp = model(θ)\n",
    "    model_score = score(tmp)\n",
    "    return (1/T) * (sum(score_deriv(tmp), x) +\n",
    "        0.5 * sum(abs2 ∘ model_score, x))\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Minimize this implicit score-matching cost function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !@isdefined(θism)\n",
    "    cost_ism1 = (θ) -> cost_ism2(data, θ)\n",
    "    opt_ism = optimize(cost_ism1, lower, upper, θ0, Fminbox(BFGS()); autodiff = :forward)\n",
    "    ##opt_ism = optimize(cost_ism1, θ0, BFGS(); autodiff = :forward)\n",
    "    θism = minimizer(opt_ism)\n",
    "    cost_ism1.([θism, θesm, θml])\n",
    "end;\n",
    "\n",
    "plot!(pf, pdf(model(θism)), label = \"ISM Gaussian mixture\", color=:cyan)\n",
    "plot!(ps, score(model(θism)), label = \"ISM score function\", color=:cyan)\n",
    "pfs = plot(pf, ps)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Curiously the supposedly equivalent ISM cost function works much worse.\n",
    "Like the ML estimate,\n",
    "the first two $σ$ values are stuck at the `lower` limit.\n",
    "Could it be local extrema?\n",
    "More investigation is needed!\n",
    "\n",
    "Ideally\n",
    "(as $T → ∞$),\n",
    "the ESM and ISM cost functions\n",
    "should differ by a constant independent of $θ$.\n",
    "Here they differ for small, finite $T$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tmp = [θ0, θesm, θml, θism]\n",
    "cost_esm1.(tmp) - cost_ism1.(tmp)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regularized score matching (RSM)\n",
    "\n",
    "[Kingma & LeCun, 2010](https://doi.org/10.5555/2997189.2997315)\n",
    "reported some instability of ISM\n",
    "and suggested a regularized version\n",
    "corresponding to the following (practical) cost function:\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{RSM}}(\\bm{θ}) =\n",
    "J_{\\mathrm{ISM}}(\\bm{θ}) + λ R(\\bm{θ})\n",
    ",\\quad\n",
    "R(\\bm{θ}) =\n",
    "\\frac{1}{T} ∑_{t=1}^T\n",
    "∑_{i=1}^N | ∂_i s_i(\\bm{x}_t; \\bm{θ}) |^2.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Regularized score matching (RSM) cost function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function cost_rsm2(x::AbstractVector{<:Real}, θ, λ)\n",
    "    mod = model(θ)\n",
    "    model_score = score(mod)\n",
    "    tmp = score_deriv(mod).(x)\n",
    "    R = sum(abs2, tmp)\n",
    "    J_ism = sum(tmp) + 0.5 * sum(abs2 ∘ model_score, x)\n",
    "    return (1/T) * (J_ism + λ * R)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Minimize this RSM cost function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "λ = 4e-1\n",
    "cost_rsm1 = (θ) -> cost_rsm2(data, θ, λ)\n",
    "\n",
    "if !@isdefined(θrsm)\n",
    "    opt_rsm = optimize(cost_rsm1, lower, upper, θ0, Fminbox(BFGS());\n",
    "        autodiff = :forward)\n",
    "    θrsm = minimizer(opt_rsm)\n",
    "    cost_rsm1.([θrsm, θ0, θism, θesm, θml])\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot!(pf, pdf(model(θism)), label = \"RSM Gaussian mixture\", color=:red)\n",
    "plot!(ps, score(model(θism)), label = \"RSM score function\", color=:red)\n",
    "pfs = plot(pf, ps)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sadly the regularized score matching (RSM) approach did not help much here.\n",
    "Increasing $λ$ led to `optimize` errors.\n",
    "\n",
    "\n",
    "## Denoising score matching (DSM)\n",
    "\n",
    "[Vincent, 2011](https://doi.org/10.1162/NECO_a_00142)\n",
    "proposed a practical approach\n",
    "called\n",
    "_denoising score matching_ (DSM)\n",
    "that matches\n",
    "the model score function\n",
    "to the score function\n",
    "of a Parzen density estimate\n",
    "of the form\n",
    "$$\n",
    "q_{σ}(\\bm{x}) = \\frac{1}{T} ∑_{t=1}^T g_{σ}(\\bm{x} - \\bm{x}_t)\n",
    "$$\n",
    "where $g_{σ}$ denotes a Gaussian distribution\n",
    "$\\mathcal{N}(\\bm{0}, σ \\bm{I})$.\n",
    "\n",
    "Statistically,\n",
    "this approach is equivalent\n",
    "(in expectation)\n",
    "to adding noise\n",
    "to the measurements,\n",
    "and then applying\n",
    "the ESM approach.\n",
    "The DSM cost function is\n",
    "$$\n",
    "J_{\\mathrm{DSM}}(\\bm{θ}) =\n",
    "\\frac{1}{T} ∑_{t=1}^T\n",
    "\\mathbb{E}_{\\bm{z} ∼ g_{σ}}\\left[\n",
    "\\frac{1}{2}\n",
    "\\left\\|\n",
    "\\bm{s}(\\bm{x}_t + \\bm{z}; \\bm{θ}) + \\frac{\\bm{z}}{σ^2}\n",
    "\\right\\|_2^2\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "A benefit of this approach\n",
    "is that it does not require\n",
    "differentiating the model score function w.r.t $\\bm{x}$.\n",
    "\n",
    "The inner expectation over $g_{σ}$\n",
    "is typically analytically intractable,\n",
    "so in practice\n",
    "we replace it with a sample mean\n",
    "where we draw $M ≥ 1$ values of $\\bm{z}$\n",
    "per training sample,\n",
    "leading to the following practical cost function\n",
    "$$\n",
    "J_{\\mathrm{DSM}, \\, M}(\\bm{θ}) =\n",
    "\\frac{1}{T} ∑_{t=1}^T\n",
    "\\frac{1}{M} ∑_{m=1}^M\n",
    "\\frac{1}{2}\n",
    "\\left\\|\n",
    "s(\\bm{x}_t + \\bm{z}_{t,m}; \\bm{θ}) + \\frac{\\bm{z}_{t,m}}{σ^2}\n",
    "\\right\\|_2^2,\n",
    "$$\n",
    "where the noise samples\n",
    "$\\bm{z}_{t,m}$\n",
    "are IID.\n",
    "\n",
    "\n",
    "The next code blocks investigate this DSM approach\n",
    "for somewhat arbitrary choices of $M$ and $σ$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "seed!(0)\n",
    "M = 9\n",
    "σdsm = 1.0\n",
    "zdsm = σdsm * randn(T, M);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define denoising score-matching cost function,\n",
    "where input `data` has size $(T,)$\n",
    "and input `z` has size $(T,M)$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function cost_dsm2(data::AbstractVector{<:Real}, z::AbstractArray{<:Real}, θ)\n",
    "    model_score = score(model(θ))\n",
    "    tmp = model_score.(data .+ z) # (T,M) # add noise to data\n",
    "    return (0.5/T/M) * sum(abs2, tmp + z ./ σdsm^2)\n",
    "end;\n",
    "\n",
    "if !@isdefined(θdsm)\n",
    "    cost_dsm1 = (θ) -> cost_dsm2(data, zdsm, θ) # + β * 0.5 * norm(θ)^2;\n",
    "    opt_dsm = optimize(cost_dsm1, lower, upper, θ0, Fminbox(BFGS());\n",
    "        autodiff = :forward)\n",
    "    θdsm = minimizer(opt_dsm)\n",
    "end;\n",
    "\n",
    "plot!(pf, pdf(model(θdsm)); label = \"DSM Gaussian mixture\", color=:orange)\n",
    "plot!(ps, score(model(θdsm)); label = \"DSM score function\", color=:orange)\n",
    "pfs = plot(pf, ps)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "At least for this case, DSM worked better than ML, ISM and RSM."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Noise-conditional score-matching (NCSM)\n",
    "\n",
    "Above we used a single noise value for DSM.\n",
    "Contemporary methods use a range of noise values\n",
    "with noise-conditional score models,\n",
    "e.g.,\n",
    "[Song et al. ICLR 2021](https://openreview.net/forum?id=PxTIG12RRHS).\n",
    "\n",
    "A representative formulation\n",
    "uses a $σ^2$-weighted expectation\n",
    "like the following:\n",
    "$$\n",
    "J_{\\mathrm{NCSM}}(\\bm{θ}) ≜\n",
    "\\mathbb{E}_{σ ∼ f(σ)}\\left[ σ^2 J_{\\mathrm{DSM}}(\\bm{θ}, σ) \\right],\n",
    "$$\n",
    "$$\n",
    "J_{\\mathrm{DSM}}(\\bm{θ}, σ) ≜\n",
    "\\frac{1}{T} ∑_{t=1}^T\n",
    "\\mathbb{E}_{\\bm{z} ∼ g_{σ}}\\left[\n",
    "\\frac{1}{2}\n",
    "\\left\\|\n",
    "\\bm{s}(\\bm{x}_t + \\bm{z}; \\bm{θ}, σ) + \\frac{\\bm{z}}{σ^2}\n",
    "\\right\\|_2^2\n",
    "\\right].\n",
    "$$\n",
    "for some distribution $f(σ)$ of noise levels.\n",
    "\n",
    "Here we use a multi-layer perceptron (MLP)\n",
    "to model the 1D noise-conditional score function\n",
    "$\\bm{s}(\\bm{x}; \\bm{θ}, σ)$.\n",
    "We use a residual approach\n",
    "with data normalization\n",
    "and the baseline score of a standard normal distribution."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function make_nnmodel(\n",
    "    data; # (2,?)\n",
    "    nweight = [2, 8, 16, 8, 1], # 2 inputs: [x, σ] for NCSM\n",
    "    μdata = mean(data),\n",
    "    σdata = std(data),\n",
    ")\n",
    "    layers = [Dense(nweight[i-1], nweight[i], Flux.gelu) for i in 2:length(nweight)]\n",
    "    # Change of variables y ≜ (x - μdata) / σ(x + z)\n",
    "    pick1(x) = transpose(x[1,:]) # extract 1D data as a row vector\n",
    "    pick2(x) = transpose(x[2,:]) # extract σ as a row vector\n",
    "    quad(σ) = sqrt(σ^2 + σdata^2) # σ(x + z) (quadrature)\n",
    "    scale1(x) = [(pick1(x) .- μdata) ./ quad.(pick2(x)); pick2(x)] # standardize\n",
    "    # The baseline score function here is just -y; use in a \"ResNet\" way:\n",
    "    tmp1 = Flux.Parallel(.-, Chain(layers...), pick1)\n",
    "    scale2(σ) = quad.(σ) / σdata^2 # \"un-standardize\" for final score w.r.t x\n",
    "    tmp2 = Flux.Parallel(.*, tmp1, scale2 ∘ pick2)\n",
    "    return Chain(scale1, tmp2)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to make data pairs suitable for NN training.\n",
    "Each use of this function's output is akin to $M$ epochs of `data`.\n",
    "Use `shuffle` in case we use mini-batches later."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function dsm_data(\n",
    "    M::Int = 9,\n",
    "    σdist = Uniform(0.2,2.0),\n",
    ")\n",
    "    σdsm = Float32.(rand(σdist, T, M))\n",
    "    z = σdsm .* randn(Float32, T, M)\n",
    "    tmp1 = shuffle(data) .+ z # (T,M)\n",
    "    tmp2 = transpose([vec(tmp1) vec(σdsm)]) # (2, T*M)\n",
    "    return (tmp2, -transpose(vec(z ./ σdsm.^2)))\n",
    "end\n",
    "\n",
    "if !@isdefined(nnmodel)\n",
    "    nnmodel = make_nnmodel(data;)\n",
    "\n",
    "    # σ^2-weighted MSE loss:\n",
    "    loss3(model, x, y) = mean(abs2, (model(x) - y) .* transpose(x[2,:])) / 2\n",
    "\n",
    "    iters = 2^9\n",
    "    dataset = [dsm_data() for i in 1:iters]\n",
    "    state1 = Flux.setup(Adam(), nnmodel)\n",
    "    @info \"begin train\"\n",
    "    @time Flux.train!(loss3, nnmodel, dataset, state1)\n",
    "    @info \"end train\"\n",
    "end\n",
    "\n",
    "nnscore = x -> nnmodel([x, 0.4])[1] # lower end of σdist range\n",
    "tmp = deepcopy(ps)\n",
    "plot!(tmp, nnscore; label = \"NN score function\", color=:blue)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The noise-conditional NN score model worked OK.\n",
    "Training \"coarse to fine\" might work better than the `Uniform` approach above."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()\n",
    "\n",
    "if false # look at the set of NN score functions\n",
    "    tmp = deepcopy(psd)\n",
    "    for s in 0.1:0.2:1.6\n",
    "        plot!(tmp, x -> nnmodel([x, s])[1]; label = \"NN score function $s\")\n",
    "    end\n",
    "    gui()\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plots of data distribution with various added noise levels"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "σlist = [0.0005, 0.05, 0.1, 0.5, 1, 5]\n",
    "nσ = length(σlist)\n",
    "pl = Array{Any}(undef, nσ)\n",
    "for (i, σ) in enumerate(σlist)\n",
    "    local mix = MixtureModel(Normal, [(d,σ) for d in data])\n",
    "    xm = range(0, 20, step = min(σ/5, 0.1))\n",
    "    local tmp = pdf(mix).(xm)\n",
    "    pl[i] = plot(xm, tmp; color = :red,\n",
    "        xaxis = (L\"x\", (0, 20), [0, gamma_mean, 18]),\n",
    "        yaxis = (L\"q_σ(x)\", [0,]), title = \"σ = $σ\",\n",
    "    )\n",
    "    # plot!(pl, xm, tmp / maximum(tmp), label = \"σ = $σ\")\n",
    "end\n",
    "plot(pl...)\n",
    "\n",
    "# Plots.savefig(\"data-qsig.pdf\")\n",
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
