var documenterSearchIndex = {"docs":
[{"location":"generated/examples/02-sample/#02-sample","page":"Sampling","title":"Sampling","text":"WIP on sampling using a score model.\n\nThis page comes from a single Julia file: 02-sample.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 02-sample.ipynb, or open it in binder here: 02-sample.ipynb.","category":"section"},{"location":"generated/examples/02-sample/#Setup","page":"Sampling","title":"Setup","text":"Packages needed here.\n\nusing MIRTjim: jim, prompt\nusing Distributions: Distribution, Normal, MixtureModel, logpdf, pdf\nimport Distributions # var, mean\nimport Distributions: logpdf, pdf\nimport ForwardDiff # derivative, gradient\nusing LaTeXStrings\nusing Printf: @sprintf\nusing Random: shuffle, seed!; seed!(0)\nusing StatsBase: mean, std\nusing Plots: Plot, plot, plot!, scatter, scatter!, histogram, quiver!\nusing Plots: @animate, gif\nimport Plots # Animation\nusing Plots: default, gui, savefig\nusing Plots.PlotMeasures: px\ndefault(label=\"\", markerstrokecolor=:auto, linewidth=2,\n labelfontsize = 14, tickfontsize = 12, legendfontsize = 14,\n)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/examples/02-sample/#Overview","page":"Sampling","title":"Overview","text":"Given a score function bms(bmx bmθ) = nabla_bmx log p(bmx bmθ) one can use Langevin dynamics to draw samples from p(bmx bmθ)","category":"section"},{"location":"generated/examples/02-sample/#Illustration","page":"Sampling","title":"Illustration","text":"Sampling from a gaussian mixture distribution.\n\nSome convenience methods\n\nlogpdf(d::Distribution) = x -> logpdf(d, x)\npdf(d::Distribution) = x -> pdf(d, x)\nderivative(f::Function) = x -> ForwardDiff.derivative(f, x)\ngradient(f::Function) = x -> ForwardDiff.gradient(f, x)\n# hessian(f::Function) = x -> ForwardDiff.hessian(f, x)\nscore(d::Distribution) = derivative(logpdf(d))\nscore_deriv(d::Distribution) = derivative(score(d)); # scalar x only\n\n\nmix = MixtureModel(Normal, [(3,1), (13,3)], [0.4, 0.6])\n\nleft_margin = 20px; bottom_margin = 10px\nxaxis = (L\"x\", (-4,24), [0, 3, 13, 20])\npmp = plot(pdf(mix); label=\"Gaussian mixture pdf\", color = :blue,\n left_margin, bottom_margin, xaxis, size=(600,300),\n yaxis = (L\"p(x)\", (0, 0.17), (0:3)*0.05),\n)\n\nscore1 = score(mix)\n\nylabel_score1 = L\"s(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\, \\log \\ p(x)\"\nps1 = plot(score1; xaxis, color=:magenta,\n size=(600,300), label = \"GMM score function\",\n yaxis = (ylabel_score1, (-5,5), -4:2:4), left_margin, bottom_margin,\n)\n\npps = plot(pmp, ps1, layout=(2,1))\n\nprompt()\n\n\nfunction sampler( ;\n    score::Function = score1,\n    T::Int = 600,\n    α0::Real = 1,\n    p0::Real = 0.99,\n    alpha::AbstractVector = (@. α0 * (p0 ^ (1:T))^2),\n    ntrial::Int = 1000, # for ph\n    beta::Real = 1,\n    seed::Int = 0,\n    init_mean::Real = Distributions.mean(mix), # todo: cheating?\n    init_std::Real = sqrt(Distributions.var(mix)),\n)\n\n    seed!(seed)\n    xrun = Matrix{Float32}(undef, ntrial, T+1)\n    xrun[:,1] = init_mean .+ init_std * randn(ntrial)\n\n    for it in 1:T\n        old = xrun[:,it]\n        αt = alpha[it]\n        xrun[:,it+1] = old + αt * score.(old) + sqrt(2*beta*αt) * randn(ntrial)\n    end\n    return xrun\nend;\n\n\nif !@isdefined(xrun) || true\n    T = 600\n    ntrial = 5000\n    xrun = sampler(; T, ntrial)\nend;\n\nntrace = 50\npsl = plot(xrun[1:ntrace,:]', xlabel=\"Iteration (t)\",\n xticks = 0:100:T,\n yaxis = (L\"x_t\", (-4,26), [0, 3, 13, 20]),\n annotate = (T, -2, \"$ntrace generated samples\", :right),\n)\n\nprompt()\n# savefig(psl, \"gmm-prior-trace-$ntrace.pdf\")\n\n\nfunction gmm_hist(it::Int)\n    ph = histogram(xrun[:,it];\n        bins = -12:0.5:36, xaxis,\n        label = \"$ntrial generated samples\", normalize = true,\n        yaxis = (L\"p(x)\", (0, 0.17), 0:0.1:0.2),\n        annotate = (-3, 0.14, \"t = $it\", :left),\n    )\n    plot!(ph, x -> pdf(mix)(x);\n         linewidth=3, color=:black, label=\"GMM Distribution\",\n    )\n    return ph\nend\n\nAnimate sampling process over time\n\nif isinteractive()\n    ph = gmm_hist(T)\nelse\n    anim = @animate for it in [1:10; 20:10:100; 200:100:T]\n        ph = gmm_hist(it)\n    # tmp = @sprintf(\"%03d\", it)\n    # savefig(ph, \"gmm-prior-sample-$ntrial,$tmp.pdf\")\n    end\n    gif(anim, \"gmm-hist.gif\", fps = 6)\nend\n\nprompt()\n# savefig(ph, \"gmm-prior-sample-$ntrial.pdf\")\n\nKernel density estimate and its score function\n\nntrain = 200\ntrain_data = rand(mix, ntrain)\ngsig = 0.9\n\nkde = MixtureModel(Normal, [(x, gsig) for x in train_data])\npkd = deepcopy(pmp)\nplot!(pkd, pdf(kde); xaxis, label=\"KDE, σ=$gsig\", widen=true, color=:green)\nscatter!(pkd, train_data, zeros(ntrain), label=\"data, N=$ntrain\", color=:black)\n\nprompt()\n# savefig(pkd, \"gmm-kde-pdf-$ntrain.pdf\")\n\npks = deepcopy(ps1)\nplot!(pks, score(kde), label=\"KDE score, σ=$gsig\", color=:green)\n\nprompt()\n# savefig(pks, \"gmm-kde-score-$ntrain.pdf\")\n\n# plot(pkd, pks; layout=(2,1))\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"methods/#Methods-list","page":"Methods","title":"Methods list","text":"","category":"section"},{"location":"methods/#Methods-usage","page":"Methods","title":"Methods usage","text":"","category":"section"},{"location":"generated/examples/01-overview/#01-overview","page":"Score Matching overview","title":"Score Matching overview","text":"This page introduces the Julia package ScoreMatching.\n\nThis page comes from a single Julia file: 01-overview.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 01-overview.ipynb, or open it in binder here: 01-overview.ipynb.","category":"section"},{"location":"generated/examples/01-overview/#Setup","page":"Score Matching overview","title":"Setup","text":"Packages needed here.\n\nusing MIRTjim: jim, prompt\nusing Distributions: Distribution, Normal, MixtureModel, logpdf, pdf\nusing Distributions: Gamma, Uniform\nimport Distributions: logpdf, pdf\nimport ForwardDiff\nusing LinearAlgebra: tr, norm\nusing LaTeXStrings\nusing Random: shuffle, seed!; seed!(0)\nusing StatsBase: mean, std\nusing Optim: optimize, BFGS, Fminbox\nimport Optim: minimizer\nimport Flux\nusing Flux: Chain, Dense, Adam\nimport Plots\nusing Plots: Plot, plot, plot!, scatter, histogram, quiver!, default, gui\nusing Plots.PlotMeasures: px\nusing InteractiveUtils: versioninfo\ndefault(label=\"\", markerstrokecolor=:auto)\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\nisinteractive() ? jim(:prompt, true) : prompt(:prompt);\nnothing #hide","category":"section"},{"location":"generated/examples/01-overview/#Overview","page":"Score Matching overview","title":"Overview","text":"Given T IID training data samples bmx_1  bmx_T  mathbbR^N, we often want to find the parameters bmθ of a model distribution p(bmx bmθ) that \"best fit\" the data.\n\nMaximum-likelihood estimation is impractical for complicated models where the normalizing constant is intractable.\n\nHyvärinen 2005 proposed an alternative called score matching that circumvents needing to find the normalizing constant by using the score function of the model distribution, defined as bms(bmx bmθ) = nabla_bmx log p(bmx bmθ)","category":"section"},{"location":"generated/examples/01-overview/#Score-functions","page":"Score Matching overview","title":"Score functions","text":"Before describing score matching methods, we first illustrate what a score function looks like.\n\nConsider the (improper) model p(bmx bmθ) = frac1Z(bmθ) mathrme^-β x_2 - x_1^α where here there are two parameters bmθ = (β α), for β  0 and α  1. The score function for this model is\n\nbms(bmx bmθ)\n=\nnabla_bmx log p(bmx bmθ)\n=\n- β nabla_bmx x_2 - x_1^α\n= α β beginbmatrix 1  -1 endbmatrix\nx_2 - x_1^α-1 mathrmsign(x_2 - x_1)\n\nThis example is related to generalized Gaussian image priors, and, for α=1, is related to total variation (TV) regularization.\n\nHere is a visualization of this 'TV' pdf and the score functions.\n\nThe quiver plot shows that the score function describe directions that ascend the prior.\n\nfunction do_quiver!(p::Plot, x, y, dx, dy; thresh=0.02, scale=0.15)\n    tmp = d -> maximum(abs, filter(!isnan, d))\n    dmax = max(tmp(dx), tmp(dy))\n    ix = 5:11:length(x)\n    iy = 5:11:length(y)\n    x, y = x .+ 0*y', 0*x .+ y'\n    x = x[ix,iy]\n    y = y[ix,iy]\n    dx = dx[ix,iy] / dmax * scale\n    dy = dy[ix,iy] / dmax * scale\n    good = @. (abs(dx) > thresh) | (abs(dy) > thresh)\n    x = x[good]\n    y = y[good]\n    dx = dx[good]\n    dy = dy[good]\n    Plots.arrow(:open, :head, 0.001, 0.001)\n    return quiver!(p, x, y, quiver=(dx,dy);\n        aspect_ratio = 1,\n        title = \"TV score quiver\",\n        color = :red,\n    )\nend;\n\nif !@isdefined(ptv)\n    α = 1.01 # fairly close to TV\n    β = 1\n    x1 = range(-1, 1, 101) * 2\n    x2 = range(-1, 1, 101) * 2\n    tv_pdf2 = @. exp(-β * abs(x2' - x1)^α) # ignoring partition constant\n    ptv0 = jim(x1, x2, tv_pdf2; title = \"'TV' pdf\", clim = (0, 1),\n        color=:cividis, xlabel = L\"x_1\", ylabel = L\"x_2\", prompt=false,\n    )\n    tv_score1 = @. β * abs(x2' - x1)^(α-1) * sign(x2' - x1)\n    ptv1 = jim(x1, x2, tv_score1; title = \"TV score₁\", prompt=false,\n        color=:cividis, xlabel = L\"x_1\", ylabel = L\"x_2\", clim = (-1,1) .* 1.2,\n    )\n    tv_score2 = @. -β * abs(x2' - x1)^(α-1) * sign(x2' - x1)\n    ptv2 = jim(x1, x2, tv_score2; title = \"TV score₂\", prompt=false,\n        color=:cividis, xlabel = L\"x_1\", ylabel = L\"x_2\", clim = (-1,1) .* 1.2,\n    )\n    ptvq = do_quiver!(deepcopy(ptv0), x1, x2, tv_score1, tv_score2)\n    ptv = plot(ptv0, ptv1, ptvq, ptv2)\n    # Plots.savefig(\"score-tv.pdf\")\nend\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Score-matching","page":"Score Matching overview","title":"Score matching","text":"The idea behind the score matching approach to model fitting is\n\nhatbmθ = arg min_bmθ\nJ(bmθ)\nqquad\nJ(bmθ) =\nfrac1T _t=1^T\n bms(bmx_t bmθ) - bms(bmx_t) ^2\n\nwhere bms(bmx bmθ) = nabla_bmx log p(bmx bmθ) is the score function of the model distribution, and bms(bmx) = nabla_bmx log p(bmx) is the score function of the (typically unknown) data distribution.\n\nVincent, 2011 calls this approach explicit score matching (ESM).","category":"section"},{"location":"generated/examples/01-overview/#1D-example","page":"Score Matching overview","title":"1D example","text":"We begin by illustrating the score function for a simple mathcalN(8 3) distribution.\n\nSome convenience methods\n\nlogpdf(d::Distribution) = x -> logpdf(d, x)\npdf(d::Distribution) = x -> pdf(d, x)\nderivative(f::Function) = x -> ForwardDiff.derivative(f, x)\ngradient(f::Function) = x -> ForwardDiff.gradient(f, x)\n# hessian(f::Function) = x -> ForwardDiff.hessian(f, x)\nscore(d::Distribution) = derivative(logpdf(d))\nscore_deriv(d::Distribution) = derivative(score(d)); # scalar x only\n\ngauss_μ = 8\ngauss_σ = 3\ngauss_disn = Expr(:call, :Normal, gauss_μ, gauss_σ)\ngauss_dist = eval(gauss_disn)\nxaxis = (L\"x\", (-1,1).*5gauss_σ .+ gauss_μ, (-3:3)*gauss_σ .+ gauss_μ)\nleft_margin = 20px; bottom_margin = 10px\npgp = plot(pdf(gauss_dist); label=\"$gauss_disn pdf\", color = :blue,\n    left_margin, bottom_margin,\n    xaxis, yaxis = (L\"p(x)\", (0, 0.15), (0:3)*0.05), size=(600,200),\n)\n\n# Plots.savefig(pgp, \"gauss-pdf.pdf\")\nprompt()\n\nylabel_score1 = L\"s(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\, \\log \\ p(x)\"\npgs = plot(derivative(logpdf(gauss_dist)); color=:red, xaxis, size=(600,200),\n    label = \"$gauss_disn score function\",\n    yaxis = (ylabel_score1, (-2,2), -3:3), left_margin, bottom_margin,\n)\n\n# Plots.savefig(pgs, \"gauss-score.pdf\")\nprompt()\n\nSame plots for a gaussian mixture model (GMM)\n\nmix = MixtureModel(Normal, [(2,1), (8,3), (16,2)], [0.3, 0.4, 0.3])\nmix = MixtureModel(Normal, [(3,1), (13,3)], [0.4, 0.6])\nxaxis = (L\"x\", (-4,24), [0, 3, 13, 20])\npmp = plot(pdf(mix); label=\"Gaussian mixture pdf\", color = :blue,\n    left_margin, bottom_margin, xaxis, size=(600,200),\n    yaxis = (L\"p(x)\", (0, 0.17), (0:3)*0.05),\n)\n\n# Plots.savefig(pmp, \"mix-pdf.pdf\")\nprompt()\n\npms = plot(derivative(logpdf(mix)); color=:red, xaxis, size=(600,200),\n    label = \"GMM score function\",\n    yaxis = (ylabel_score1, (-5,5), -4:2:4), left_margin, bottom_margin,\n)\n\n# Plots.savefig(pms, \"mix-score.pdf\")\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Illustration","page":"Score Matching overview","title":"Illustration","text":"For didactic purposes, we illustrate explicit score matching (ESM) by fitting samples from a Gamma distribution to a mixture of gaussians.\n\nGenerate training data\n\nif !@isdefined(data)\n    T = 100\n    gamma_k = 8 # shape\n    gamma_θ = 1 # scale\n    gamma_mode = gamma_k > 1 ? (gamma_k - 1) * gamma_θ : 0\n    gamma_mean = gamma_k * gamma_θ\n    gamma_std = sqrt(gamma_k) * gamma_θ\n    data_disn = Expr(:call, :Gamma, gamma_k, gamma_θ)\n    data_dis = eval(data_disn)\n    data_score = derivative(logpdf(data_dis))\n    data = Float32.(rand(data_dis, T))\n    xlims = (-1, 25)\n    xticks = [0, floor(Int, minimum(data)), gamma_mode, gamma_mean, ceil(Int, maximum(data))]\n    xticks = sort(xticks) # ticks that span the data range\n\n    pfd = scatter(data, zeros(T); xlims, xticks, color=:black)\n    plot!(pfd, pdf(data_dis); label=\"$data_disn pdf\",\n        color = :black, xlabel = L\"x\", ylabel = L\"p(x)\")\n\n    psd = plot(data_score; color=:black,\n        label = \"$(data_disn.args[1]) score function\",\n        xaxis=(L\"x\", (1,20), xticks), ylims = (-3, 5), yticks=[0,4],\n    )\n    psdn = deepcopy(psd)\n    tmp = score(Normal(mean(data), std(data)))\n    plot!(psdn, tmp; label = \"Normal score function\", line=:dash, color=:black)\n\n    ph = histogram(data; linecolor=:blue,\n        xlabel=L\"x\", size=(600,300), yaxis=(\"count\", (0,15), 0:5:15),\n        bins=-1:0.5:25, xlims, xticks, label=\"data histogram\")\n    # Plots.savefig(ph, \"gamma-data.pdf\")\n    plot!(ph, x -> T*0.5 * pdf(data_dis)(x);\n        color=:black, label=\"$data_disn Distribution\")\n    # Plots.savefig(ph, \"gamma-fit.pdf\")\nend\n\n\nif false # plots for a talk\n    pdt = plot(pdf(data_dis); label=\"$data_disn pdf\", color = :blue,\n        left_margin, bottom_margin,\n        xaxis = (L\"x\", xlims, xticks), ylabel = L\"p(x)\", size=(600,200))\n    pst = deepcopy(psd)\n    tmp = score(Normal(gamma_mean, gamma_std))\n    plot!(pst, tmp; label = \"Normal score function\", size=(600,200), xlims,\n        line=:dash, color=:magenta, left_margin, bottom_margin,\n        ylabel=ylabel_score1)\n\n    # Plots.savefig(pdt, \"gamma-pdf.pdf\")\n    # Plots.savefig(pst, \"gamma-score.pdf\")\nend\n\nTo perform unconstrained minimization of a D-component mixture, the following mapping from mathbbR^D-1 to the D-dimensional simplex is helpful. It is the inverse of the additive logratio transform. It is related to the softmax function.\n\nfunction map_r_s(y::AbstractVector; scale::Real = 1.0)\n    y = scale * [y; 0]\n    y .-= maximum(y) # for numerical stability\n    p = exp.(y)\n    return p / sum(p)\nend\nmap_r_s(y::Real...) = map_r_s([y...])\n\ny1 = range(-1,1,101) * 9\ny2 = range(-1,1,101) * 9\ntmp = map_r_s.(y1, y2')\npj = jim(y1, y2, tmp; title=\"Simplex parameterization\", nrow=1)\n\nDefine model distribution\n\nnmix = 3 # how many gaussians in the mixture model\nfunction model(θ ;\n    σmin::Real = 1,\n    σmax::Real = 19,\n)\n    mu = θ[1:nmix]\n    sig = θ[nmix .+ (1:nmix)]\n    any(<(σmin), sig) && throw(\"too small σ\")\n    any(>(σmax), sig) && throw(\"too big σ $sig\")\n    # sig = σmin .+ exp.(sig) # ensure σ > 0\n    # sig = @. σmin + (σmax - σmin) * (tanh(sig/2) + 1) / 2 # \"constraints\"\n    p = map_r_s(θ[2nmix .+ (1:(nmix-1))])\n    tmp = [(μ,σ) for (μ,σ) in zip(mu, sig)]\n    mix = MixtureModel(Normal, tmp, p)\n    return mix\nend;\nnothing #hide\n\nDefine explicit score-matching cost function\n\nfunction cost_esm2(x::AbstractVector{<:Real}, θ)\n    model_score = score(model(θ))\n    return (0.5/T) * sum(abs2, model_score.(x) - data_score.(x))\nend;\nnothing #hide\n\nMinimize this explicit score-matching cost function:\n\nβ = 0e-4 # optional small regularizer to ensure coercive\ncost_esm1 = (θ) -> cost_esm2(data, θ) + β * 0.5 * norm(θ)^2;\nnothing #hide\n\nInitial crude guess of mixture model parameters\n\nθ0 = Float64[5, 7, 9, 1.5, 1.5, 1.5, 0, 0]; # Gamma\nnothing #hide\n\nPlot data pdf and initial model pdf\n\npf = deepcopy(pfd)\nplot!(pf, pdf(model(θ0)), label = \"Initial Gaussian mixture\", color=:blue)\n\nprompt()\n\nCheck descent and non-convexity\n\nif false\n    tmp = gradient(cost_esm1)(θ0)\n    a = range(0, 9, 101)\n    h = a -> cost_esm1(θ0 - a * tmp)\n    plot(a, log.(h.(a)))\nend","category":"section"},{"location":"generated/examples/01-overview/#Explicit-score-matching-(ESM)-(impractical)","page":"Score Matching overview","title":"Explicit score matching (ESM) (impractical)","text":"if !@isdefined(θesm)\n    lower = [fill(0, nmix); fill(1.0, nmix); fill(-Inf, nmix-1)]\n    upper = [fill(Inf, nmix); fill(Inf, nmix); fill(Inf, nmix-1)]\n    opt_esm = optimize(cost_esm1, lower, upper, θ0, Fminbox(BFGS());\n        autodiff = :forward)\n    # opt_esm = optimize(cost_esm1, θ0, BFGS(); autodiff = :forward) # unconstrained\n    θesm = minimizer(opt_esm)\nend;\n\nplot!(pf, pdf(model(θesm)), label = \"ESM Gaussian mixture\", color=:green)\n\nprompt()\n\nPlot the data score and model score functions to see how well they match. The largest mismatch is in the tails of the distribution where there are few (if any) data points.\n\nps = deepcopy(psd)\nplot!(ps, score(model(θesm)); label = \"ESM score function\", color=:green)\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Maximum-likelihood-estimation","page":"Score Matching overview","title":"Maximum-likelihood estimation","text":"This toy example is simple enough that we can apply ML estimation to it directly. In fact, ML estimation is a seemingly more practical optimization problem than score matching in this case.\n\nAs expected, ML estimation leads to a lower negative log-likelihood.\n\nnegloglike(θ) = (-1/T) * sum(logpdf(model(θ)), data)\nopt_ml = optimize(negloglike, lower, upper, θ0, Fminbox(BFGS()); autodiff = :forward)\nθml = minimizer(opt_ml)\nnegloglike.([θml, θesm, θ0])\n\nCuriously, ML estimation here leads to much worse fits to the pdf than score matching, even though we initialized the ML optimizer with the score-matching parameters. Perhaps the landscape of the log-likelihood is less well-behaved than that of the SM cost.\n\nplot!(pf, pdf(model(θml)), label = \"ML Gaussian mixture\", color=:magenta)\nplot!(ps, score(model(θml)), label = \"ML score function\", color=:magenta)\nplot(pf, ps)\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Implicit-score-matching-(ISM)-(more-practical)","page":"Score Matching overview","title":"Implicit score matching (ISM) (more practical)","text":"The above ESM fitting process used score(data_dis), the score-function of the data distribution, which is unknown in practical situations.\n\nHyvärinen 2005 derived the following more practical cost function that is independent of the unknown data score function:\n\nJ_mathrmISM(bmθ) =\nfrac1T _t=1^T\n_i=1^N _i s_i(bmx_t bmθ)\n + frac12  s_i(bmx_t bmθ) ^2\n\nignoring a constant that is independent of θ where\n\n_i s_i(bmx bmθ)\n=\nfrac x_i s_i(bmx bmθ)\n=\nfrac^2 x_i^2 log p(bmx bmθ)\n\n(For large models this version is still a bit impractical because it depends on the diagonal elements of the Hessian of the log prior. Subsequent pages deal with that issue.)\n\nVincent, 2011 calls this approach implicit score matching (ISM).\n\nImplicit score-matching cost function\n\nfunction cost_ism2(x::AbstractVector{<:Real}, θ)\n    tmp = model(θ)\n    model_score = score(tmp)\n    return (1/T) * (sum(score_deriv(tmp), x) +\n        0.5 * sum(abs2 ∘ model_score, x))\nend;\nnothing #hide\n\nMinimize this implicit score-matching cost function:\n\nif !@isdefined(θism)\n    cost_ism1 = (θ) -> cost_ism2(data, θ)\n    opt_ism = optimize(cost_ism1, lower, upper, θ0, Fminbox(BFGS()); autodiff = :forward)\n    ##opt_ism = optimize(cost_ism1, θ0, BFGS(); autodiff = :forward)\n    θism = minimizer(opt_ism)\n    cost_ism1.([θism, θesm, θml])\nend;\n\nplot!(pf, pdf(model(θism)), label = \"ISM Gaussian mixture\", color=:cyan)\nplot!(ps, score(model(θism)), label = \"ISM score function\", color=:cyan)\npfs = plot(pf, ps)\n\nprompt()\n\nCuriously the supposedly equivalent ISM cost function works much worse. Like the ML estimate, the first two σ values are stuck at the lower limit. Could it be local extrema? More investigation is needed!\n\nIdeally (as T  ), the ESM and ISM cost functions should differ by a constant independent of θ. Here they differ for small, finite T.\n\ntmp = [θ0, θesm, θml, θism]\ncost_esm1.(tmp) - cost_ism1.(tmp)","category":"section"},{"location":"generated/examples/01-overview/#Regularized-score-matching-(RSM)","page":"Score Matching overview","title":"Regularized score matching (RSM)","text":"Kingma & LeCun, 2010 reported some instability of ISM and suggested a regularized version corresponding to the following (practical) cost function:\n\nJ_mathrmRSM(bmθ) =\nJ_mathrmISM(bmθ) + λ R(bmθ)\nquad\nR(bmθ) =\nfrac1T _t=1^T\n_i=1^N  _i s_i(bmx_t bmθ) ^2\n\nRegularized score matching (RSM) cost function\n\nfunction cost_rsm2(x::AbstractVector{<:Real}, θ, λ)\n    mod = model(θ)\n    model_score = score(mod)\n    tmp = score_deriv(mod).(x)\n    R = sum(abs2, tmp)\n    J_ism = sum(tmp) + 0.5 * sum(abs2 ∘ model_score, x)\n    return (1/T) * (J_ism + λ * R)\nend;\nnothing #hide\n\nMinimize this RSM cost function:\n\nλ = 4e-1\ncost_rsm1 = (θ) -> cost_rsm2(data, θ, λ)\n\nif !@isdefined(θrsm)\n    opt_rsm = optimize(cost_rsm1, lower, upper, θ0, Fminbox(BFGS());\n        autodiff = :forward)\n    θrsm = minimizer(opt_rsm)\n    cost_rsm1.([θrsm, θ0, θism, θesm, θml])\nend\n\nplot!(pf, pdf(model(θism)), label = \"RSM Gaussian mixture\", color=:red)\nplot!(ps, score(model(θism)), label = \"RSM score function\", color=:red)\npfs = plot(pf, ps)\n\nprompt()\n\nSadly the regularized score matching (RSM) approach did not help much here. Increasing λ led to optimize errors.","category":"section"},{"location":"generated/examples/01-overview/#Denoising-score-matching-(DSM)","page":"Score Matching overview","title":"Denoising score matching (DSM)","text":"Vincent, 2011 proposed a practical approach called denoising score matching (DSM) that matches the model score function to the score function of a Parzen density estimate of the form\n\nq_σ(bmx) = frac1T _t=1^T g_σ(bmx - bmx_t)\n\nwhere g_σ denotes a Gaussian distribution mathcalN(bm0 σ bmI).\n\nStatistically, this approach is equivalent (in expectation) to adding noise to the measurements, and then applying the ESM approach. The DSM cost function is\n\nJ_mathrmDSM(bmθ) =\nfrac1T _t=1^T\nmathbbE_bmz  g_σleft\nfrac12\nleft\nbms(bmx_t + bmz bmθ) + fracbmzσ^2\nright_2^2\nright\n\nA benefit of this approach is that it does not require differentiating the model score function w.r.t bmx.\n\nThe inner expectation over g_σ is typically analytically intractable, so in practice we replace it with a sample mean where we draw M  1 values of bmz per training sample, leading to the following practical cost function\n\nJ_mathrmDSM  M(bmθ) =\nfrac1T _t=1^T\nfrac1M _m=1^M\nfrac12\nleft\ns(bmx_t + bmz_tm bmθ) + fracbmz_tmσ^2\nright_2^2\n\nwhere the noise samples bmz_tm are IID.\n\nThe next code blocks investigate this DSM approach for somewhat arbitrary choices of M and σ.\n\nseed!(0)\nM = 9\nσdsm = 1.0\nzdsm = σdsm * randn(T, M);\nnothing #hide\n\nDefine denoising score-matching cost function, where input data has size (T) and input z has size (TM).\n\nfunction cost_dsm2(data::AbstractVector{<:Real}, z::AbstractArray{<:Real}, θ)\n    model_score = score(model(θ))\n    tmp = model_score.(data .+ z) # (T,M) # add noise to data\n    return (0.5/T/M) * sum(abs2, tmp + z ./ σdsm^2)\nend;\n\nif !@isdefined(θdsm)\n    cost_dsm1 = (θ) -> cost_dsm2(data, zdsm, θ) # + β * 0.5 * norm(θ)^2;\n    opt_dsm = optimize(cost_dsm1, lower, upper, θ0, Fminbox(BFGS());\n        autodiff = :forward)\n    θdsm = minimizer(opt_dsm)\nend;\n\nplot!(pf, pdf(model(θdsm)); label = \"DSM Gaussian mixture\", color=:orange)\nplot!(ps, score(model(θdsm)); label = \"DSM score function\", color=:orange)\npfs = plot(pf, ps)\n\nAt least for this case, DSM worked better than ML, ISM and RSM.\n\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Noise-conditional-score-matching-(NCSM)","page":"Score Matching overview","title":"Noise-conditional score-matching (NCSM)","text":"Above we used a single noise value for DSM. Contemporary methods use a range of noise values with noise-conditional score models, e.g., Song et al. ICLR 2021.\n\nA representative formulation uses a σ^2-weighted expectation like the following:\n\nJ_mathrmNCSM(bmθ) \nmathbbE_σ  f(σ)left σ^2 J_mathrmDSM(bmθ σ) right\n\nJ_mathrmDSM(bmθ σ) \nfrac1T _t=1^T\nmathbbE_bmz  g_σleft\nfrac12\nleft\nbms(bmx_t + bmz bmθ σ) + fracbmzσ^2\nright_2^2\nright\n\nfor some distribution f(σ) of noise levels.\n\nHere we use a multi-layer perceptron (MLP) to model the 1D noise-conditional score function bms(bmx bmθ σ). We use a residual approach with data normalization and the baseline score of a standard normal distribution.\n\nfunction make_nnmodel(\n    data; # (2,?)\n    nweight = [2, 8, 16, 8, 1], # 2 inputs: [x, σ] for NCSM\n    μdata = mean(data),\n    σdata = std(data),\n)\n    layers = [Dense(nweight[i-1], nweight[i], Flux.gelu) for i in 2:length(nweight)]\n    # Change of variables y ≜ (x - μdata) / σ(x + z)\n    pick1(x) = transpose(x[1,:]) # extract 1D data as a row vector\n    pick2(x) = transpose(x[2,:]) # extract σ as a row vector\n    quad(σ) = sqrt(σ^2 + σdata^2) # σ(x + z) (quadrature)\n    scale1(x) = [(pick1(x) .- μdata) ./ quad.(pick2(x)); pick2(x)] # standardize\n    # The baseline score function here is just -y; use in a \"ResNet\" way:\n    tmp1 = Flux.Parallel(.-, Chain(layers...), pick1)\n    scale2(σ) = quad.(σ) / σdata^2 # \"un-standardize\" for final score w.r.t x\n    tmp2 = Flux.Parallel(.*, tmp1, scale2 ∘ pick2)\n    return Chain(scale1, tmp2)\nend;\nnothing #hide\n\nFunction to make data pairs suitable for NN training. Each use of this function's output is akin to M epochs of data. Use shuffle in case we use mini-batches later.\n\nfunction dsm_data(\n    M::Int = 9,\n    σdist = Uniform(0.2,2.0),\n)\n    σdsm = Float32.(rand(σdist, T, M))\n    z = σdsm .* randn(Float32, T, M)\n    tmp1 = shuffle(data) .+ z # (T,M)\n    tmp2 = transpose([vec(tmp1) vec(σdsm)]) # (2, T*M)\n    return (tmp2, -transpose(vec(z ./ σdsm.^2)))\nend\n\nif !@isdefined(nnmodel)\n    nnmodel = make_nnmodel(data;)\n\n    # σ^2-weighted MSE loss:\n    loss3(model, x, y) = mean(abs2, (model(x) - y) .* transpose(x[2,:])) / 2\n\n    iters = 2^9\n    dataset = [dsm_data() for i in 1:iters]\n    state1 = Flux.setup(Adam(), nnmodel)\n    @info \"begin train\"\n    @time Flux.train!(loss3, nnmodel, dataset, state1)\n    @info \"end train\"\nend\n\nnnscore = x -> nnmodel([x, 0.4])[1] # lower end of σdist range\ntmp = deepcopy(ps)\nplot!(tmp, nnscore; label = \"NN score function\", color=:blue)\n\nThe noise-conditional NN score model worked OK. Training \"coarse to fine\" might work better than the Uniform approach above.\n\nprompt()\n\nif false # look at the set of NN score functions\n    tmp = deepcopy(psd)\n    for s in 0.1:0.2:1.6\n        plot!(tmp, x -> nnmodel([x, s])[1]; label = \"NN score function $s\")\n    end\n    gui()\nend\n\nPlots of data distribution with various added noise levels\n\nσlist = [0.0005, 0.05, 0.1, 0.5, 1, 5]\nnσ = length(σlist)\npl = Array{Any}(undef, nσ)\nfor (i, σ) in enumerate(σlist)\n    local mix = MixtureModel(Normal, [(d,σ) for d in data])\n    xm = range(0, 20, step = min(σ/5, 0.1))\n    local tmp = pdf(mix).(xm)\n    pl[i] = plot(xm, tmp; color = :red,\n        xaxis = (L\"x\", (0, 20), [0, gamma_mean, 18]),\n        yaxis = (L\"q_σ(x)\", [0,]), title = \"σ = $σ\",\n    )\n    # plot!(pl, xm, tmp / maximum(tmp), label = \"σ = $σ\")\nend\nplot(pl...)\n\n# Plots.savefig(\"data-qsig.pdf\")\nprompt()","category":"section"},{"location":"generated/examples/01-overview/#Reproducibility","page":"Score Matching overview","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#ScoreMatching.jl-Documentation","page":"Home","title":"ScoreMatching.jl Documentation","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"This Julia package ScoreMatching provides tools for fitting statistical distributions to training data.\n\nHere, \"score\" means the gradient of a log-likelihood nabla_mathbfx log p(mathbfx mathbftheta) as defined  by Hyvärinen 2005.\n\nSee the \"Examples\" for details.","category":"section"}]
}
