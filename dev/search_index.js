var documenterSearchIndex = {"docs":
[{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"EditURL = \"../../../lit/examples/02-sample.jl\"","category":"page"},{"location":"generated/examples/02-sample/#02-sample","page":"Sampling","title":"Sampling","text":"","category":"section"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"WIP on sampling using a score model.","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"This page comes from a single Julia file: 02-sample.jl.","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"You can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 02-sample.ipynb, or open it in binder here: 02-sample.ipynb.","category":"page"},{"location":"generated/examples/02-sample/#Setup","page":"Sampling","title":"Setup","text":"","category":"section"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"Packages needed here.","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"using MIRTjim: jim, prompt\nusing Distributions: Distribution, Normal, MixtureModel, logpdf, pdf\nimport Distributions # var, mean\nimport Distributions: logpdf, pdf\nimport ForwardDiff # derivative, gradient\nusing LaTeXStrings\nusing Printf: @sprintf\nusing Random: shuffle, seed!; seed!(0)\nusing StatsBase: mean, std\nusing Plots: Plot, plot, plot!, scatter, scatter!, histogram, quiver!\nusing Plots: @animate, gif\nimport Plots # Animation\nusing Plots: default, gui, savefig\nusing Plots.PlotMeasures: px\ndefault(label=\"\", markerstrokecolor=:auto, linewidth=2,\n labelfontsize = 14, tickfontsize = 12, legendfontsize = 14,\n)","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"The following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"isinteractive() ? jim(:prompt, true) : prompt(:prompt);\nnothing #hide","category":"page"},{"location":"generated/examples/02-sample/#Overview","page":"Sampling","title":"Overview","text":"","category":"section"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"Given a score function bms(bmx bmθ) = nabla_bmx log p(bmx bmθ) one can use Langevin dynamics to draw samples from p(bmx bmθ)","category":"page"},{"location":"generated/examples/02-sample/#Illustration","page":"Sampling","title":"Illustration","text":"","category":"section"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"Sampling from a gaussian mixture distribution.","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"Some convenience methods","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"logpdf(d::Distribution) = x -> logpdf(d, x)\npdf(d::Distribution) = x -> pdf(d, x)\nderivative(f::Function) = x -> ForwardDiff.derivative(f, x)\ngradient(f::Function) = x -> ForwardDiff.gradient(f, x)\n# hessian(f::Function) = x -> ForwardDiff.hessian(f, x)\nscore(d::Distribution) = derivative(logpdf(d))\nscore_deriv(d::Distribution) = derivative(score(d)); # scalar x only\n\n\nmix = MixtureModel(Normal, [(3,1), (13,3)], [0.4, 0.6])\n\nleft_margin = 20px; bottom_margin = 10px\nxaxis = (L\"x\", (-4,24), [0, 3, 13, 20])\npmp = plot(pdf(mix); label=\"Gaussian mixture pdf\", color = :blue,\n left_margin, bottom_margin, xaxis, size=(600,300),\n yaxis = (L\"p(x)\", (0, 0.17), (0:3)*0.05),\n)\n\nscore1 = score(mix)\n\nylabel_score1 = L\"s(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\, \\log \\ p(x)\"\nps1 = plot(score1; xaxis, color=:magenta,\n size=(600,300), label = \"GMM score function\",\n yaxis = (ylabel_score1, (-5,5), -4:2:4), left_margin, bottom_margin,\n)\n\npps = plot(pmp, ps1, layout=(2,1))","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"prompt()\n\n\nfunction sampler( ;\n    score::Function = score1,\n    T::Int = 600,\n    α0::Real = 1,\n    p0::Real = 0.99,\n    alpha::AbstractVector = (@. α0 * (p0 ^ (1:T))^2),\n    ntrial::Int = 1000, # for ph\n    beta::Real = 1,\n    seed::Int = 0,\n    init_mean::Real = Distributions.mean(mix), # todo: cheating?\n    init_std::Real = sqrt(Distributions.var(mix)),\n)\n\n    seed!(seed)\n    xrun = Matrix{Float32}(undef, ntrial, T+1)\n    xrun[:,1] = init_mean .+ init_std * randn(ntrial)\n\n    for it in 1:T\n        old = xrun[:,it]\n        αt = alpha[it]\n        xrun[:,it+1] = old + αt * score.(old) + sqrt(2*beta*αt) * randn(ntrial)\n    end\n    return xrun\nend;\n\n\nif !@isdefined(xrun) || true\n    T = 600\n    ntrial = 5000\n    xrun = sampler(; T, ntrial)\nend;\n\nntrace = 50\npsl = plot(xrun[1:ntrace,:]', xlabel=\"Iteration (t)\",\n xticks = 0:100:T,\n yaxis = (L\"x_t\", (-4,26), [0, 3, 13, 20]),\n annotate = (T, -2, \"$ntrace generated samples\", :right),\n)","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"prompt()\n# savefig(psl, \"gmm-prior-trace-$ntrace.pdf\")\n\n\nfunction gmm_hist(it::Int)\n    ph = histogram(xrun[:,it];\n        bins = -12:0.5:36, xaxis,\n        label = \"$ntrial generated samples\", normalize = true,\n        yaxis = (L\"p(x)\", (0, 0.17), 0:0.1:0.2),\n        annotate = (-3, 0.14, \"t = $it\", :left),\n    )\n    plot!(ph, x -> pdf(mix)(x);\n         linewidth=3, color=:black, label=\"GMM Distribution\",\n    )\n    return ph\nend","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"Animate sampling process over time","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"if isinteractive()\n    ph = gmm_hist(T)\nelse\n    anim = @animate for it in [1:10; 20:10:100; 200:100:T]\n        ph = gmm_hist(it)\n    # tmp = @sprintf(\"%03d\", it)\n    # savefig(ph, \"gmm-prior-sample-$ntrial,$tmp.pdf\")\n    end\n    gif(anim, \"gmm-hist.gif\", fps = 6)\nend","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"prompt()\n# savefig(ph, \"gmm-prior-sample-$ntrial.pdf\")","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"Kernel density estimate and its score function","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"ntrain = 200\ntrain_data = rand(mix, ntrain)\ngsig = 0.9\n\nkde = MixtureModel(Normal, [(x, gsig) for x in train_data])\npkd = deepcopy(pmp)\nplot!(pkd, pdf(kde); xaxis, label=\"KDE, σ=$gsig\", widen=true, color=:green)\nscatter!(pkd, train_data, zeros(ntrain), label=\"data, N=$ntrain\", color=:black)","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"prompt()\n# savefig(pkd, \"gmm-kde-pdf-$ntrain.pdf\")\n\npks = deepcopy(ps1)\nplot!(pks, score(kde), label=\"KDE score, σ=$gsig\", color=:green)","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"prompt()\n# savefig(pks, \"gmm-kde-score-$ntrain.pdf\")\n\n# plot(pkd, pks; layout=(2,1))","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"","category":"page"},{"location":"generated/examples/02-sample/","page":"Sampling","title":"Sampling","text":"This page was generated using Literate.jl.","category":"page"},{"location":"methods/#Methods-list","page":"Methods","title":"Methods list","text":"","category":"section"},{"location":"methods/","page":"Methods","title":"Methods","text":"","category":"page"},{"location":"methods/#Methods-usage","page":"Methods","title":"Methods usage","text":"","category":"section"},{"location":"methods/","page":"Methods","title":"Methods","text":"Modules = [ScoreMatching]","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"EditURL = \"../../../lit/examples/01-overview.jl\"","category":"page"},{"location":"generated/examples/01-overview/#01-overview","page":"Score Matching overview","title":"Score Matching overview","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"This page introduces the Julia package ScoreMatching.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"This page comes from a single Julia file: 01-overview.jl.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"You can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 01-overview.ipynb, or open it in binder here: 01-overview.ipynb.","category":"page"},{"location":"generated/examples/01-overview/#Setup","page":"Score Matching overview","title":"Setup","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Packages needed here.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"using MIRTjim: jim, prompt\nusing Distributions: Distribution, Normal, MixtureModel, logpdf, pdf\nusing Distributions: Gamma, Uniform\nimport Distributions: logpdf, pdf\nimport ForwardDiff\nusing LinearAlgebra: tr, norm\nusing LaTeXStrings\nusing Random: shuffle, seed!; seed!(0)\nusing StatsBase: mean, std\nusing Optim: optimize, BFGS, Fminbox\nimport Optim: minimizer\nimport Flux\nusing Flux: Chain, Dense, Adam\nimport Plots\nusing Plots: Plot, plot, plot!, scatter, histogram, quiver!, default, gui\nusing Plots.PlotMeasures: px\nusing InteractiveUtils: versioninfo\ndefault(label=\"\", markerstrokecolor=:auto)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"The following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"isinteractive() ? jim(:prompt, true) : prompt(:prompt);\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/#Overview","page":"Score Matching overview","title":"Overview","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Given T IID training data samples bmx_1  bmx_T  mathbbR^N, we often want to find the parameters bmθ of a model distribution p(bmx bmθ) that \"best fit\" the data.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Maximum-likelihood estimation is impractical for complicated models where the normalizing constant is intractable.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Hyvärinen 2005 proposed an alternative called score matching that circumvents needing to find the normalizing constant by using the score function of the model distribution, defined as bms(bmx bmθ) = nabla_bmx log p(bmx bmθ)","category":"page"},{"location":"generated/examples/01-overview/#Score-functions","page":"Score Matching overview","title":"Score functions","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Before describing score matching methods, we first illustrate what a score function looks like.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Consider the (improper) model p(bmx bmθ) = frac1Z(bmθ) mathrme^-β x_2 - x_1^α where here there are two parameters bmθ = (β α), for β  0 and α  1. The score function for this model is","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"bms(bmx bmθ)\n=\nnabla_bmx log p(bmx bmθ)\n=\n- β nabla_bmx x_2 - x_1^α\n= α β beginbmatrix 1  -1 endbmatrix\nx_2 - x_1^α-1 mathrmsign(x_2 - x_1)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"This example is related to generalized Gaussian image priors, and, for α=1, is related to total variation (TV) regularization.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Here is a visualization of this 'TV' pdf and the score functions.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"The quiver plot shows that the score function describe directions that ascend the prior.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"function do_quiver!(p::Plot, x, y, dx, dy; thresh=0.02, scale=0.15)\n    tmp = d -> maximum(abs, filter(!isnan, d))\n    dmax = max(tmp(dx), tmp(dy))\n    ix = 5:11:length(x)\n    iy = 5:11:length(y)\n    x, y = x .+ 0*y', 0*x .+ y'\n    x = x[ix,iy]\n    y = y[ix,iy]\n    dx = dx[ix,iy] / dmax * scale\n    dy = dy[ix,iy] / dmax * scale\n    good = @. (abs(dx) > thresh) | (abs(dy) > thresh)\n    x = x[good]\n    y = y[good]\n    dx = dx[good]\n    dy = dy[good]\n    Plots.arrow(:open, :head, 0.001, 0.001)\n    return quiver!(p, x, y, quiver=(dx,dy);\n        aspect_ratio = 1,\n        title = \"TV score quiver\",\n        color = :red,\n    )\nend;\n\nif !@isdefined(ptv)\n    α = 1.01 # fairly close to TV\n    β = 1\n    x1 = range(-1, 1, 101) * 2\n    x2 = range(-1, 1, 101) * 2\n    tv_pdf2 = @. exp(-β * abs(x2' - x1)^α) # ignoring partition constant\n    ptv0 = jim(x1, x2, tv_pdf2; title = \"'TV' pdf\", clim = (0, 1),\n        color=:cividis, xlabel = L\"x_1\", ylabel = L\"x_2\", prompt=false,\n    )\n    tv_score1 = @. β * abs(x2' - x1)^(α-1) * sign(x2' - x1)\n    ptv1 = jim(x1, x2, tv_score1; title = \"TV score₁\", prompt=false,\n        color=:cividis, xlabel = L\"x_1\", ylabel = L\"x_2\", clim = (-1,1) .* 1.2,\n    )\n    tv_score2 = @. -β * abs(x2' - x1)^(α-1) * sign(x2' - x1)\n    ptv2 = jim(x1, x2, tv_score2; title = \"TV score₂\", prompt=false,\n        color=:cividis, xlabel = L\"x_1\", ylabel = L\"x_2\", clim = (-1,1) .* 1.2,\n    )\n    ptvq = do_quiver!(deepcopy(ptv0), x1, x2, tv_score1, tv_score2)\n    ptv = plot(ptv0, ptv1, ptvq, ptv2)\n    # Plots.savefig(\"score-tv.pdf\")\nend","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/#Score-matching","page":"Score Matching overview","title":"Score matching","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"The idea behind the score matching approach to model fitting is","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"hatbmθ = arg min_bmθ\nJ(bmθ)\nqquad\nJ(bmθ) =\nfrac1T _t=1^T\n bms(bmx_t bmθ) - bms(bmx_t) ^2","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"where bms(bmx bmθ) = nabla_bmx log p(bmx bmθ) is the score function of the model distribution, and bms(bmx) = nabla_bmx log p(bmx) is the score function of the (typically unknown) data distribution.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Vincent, 2011 calls this approach explicit score matching (ESM).","category":"page"},{"location":"generated/examples/01-overview/#1D-example","page":"Score Matching overview","title":"1D example","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"We begin by illustrating the score function for a simple mathcalN(8 3) distribution.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Some convenience methods","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"logpdf(d::Distribution) = x -> logpdf(d, x)\npdf(d::Distribution) = x -> pdf(d, x)\nderivative(f::Function) = x -> ForwardDiff.derivative(f, x)\ngradient(f::Function) = x -> ForwardDiff.gradient(f, x)\n# hessian(f::Function) = x -> ForwardDiff.hessian(f, x)\nscore(d::Distribution) = derivative(logpdf(d))\nscore_deriv(d::Distribution) = derivative(score(d)); # scalar x only\n\ngauss_μ = 8\ngauss_σ = 3\ngauss_disn = Expr(:call, :Normal, gauss_μ, gauss_σ)\ngauss_dist = eval(gauss_disn)\nxaxis = (L\"x\", (-1,1).*5gauss_σ .+ gauss_μ, (-3:3)*gauss_σ .+ gauss_μ)\nleft_margin = 20px; bottom_margin = 10px\npgp = plot(pdf(gauss_dist); label=\"$gauss_disn pdf\", color = :blue,\n    left_margin, bottom_margin,\n    xaxis, yaxis = (L\"p(x)\", (0, 0.15), (0:3)*0.05), size=(600,200),\n)\n\n# Plots.savefig(pgp, \"gauss-pdf.pdf\")\nprompt()\n\nylabel_score1 = L\"s(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\, \\log \\ p(x)\"\npgs = plot(derivative(logpdf(gauss_dist)); color=:red, xaxis, size=(600,200),\n    label = \"$gauss_disn score function\",\n    yaxis = (ylabel_score1, (-2,2), -3:3), left_margin, bottom_margin,\n)\n\n# Plots.savefig(pgs, \"gauss-score.pdf\")\nprompt()","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Same plots for a gaussian mixture model (GMM)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"mix = MixtureModel(Normal, [(2,1), (8,3), (16,2)], [0.3, 0.4, 0.3])\nmix = MixtureModel(Normal, [(3,1), (13,3)], [0.4, 0.6])\nxaxis = (L\"x\", (-4,24), [0, 3, 13, 20])\npmp = plot(pdf(mix); label=\"Gaussian mixture pdf\", color = :blue,\n    left_margin, bottom_margin, xaxis, size=(600,200),\n    yaxis = (L\"p(x)\", (0, 0.17), (0:3)*0.05),\n)\n\n# Plots.savefig(pmp, \"mix-pdf.pdf\")\nprompt()\n\npms = plot(derivative(logpdf(mix)); color=:red, xaxis, size=(600,200),\n    label = \"GMM score function\",\n    yaxis = (ylabel_score1, (-5,5), -4:2:4), left_margin, bottom_margin,\n)\n\n# Plots.savefig(pms, \"mix-score.pdf\")\nprompt()","category":"page"},{"location":"generated/examples/01-overview/#Illustration","page":"Score Matching overview","title":"Illustration","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"For didactic purposes, we illustrate explicit score matching (ESM) by fitting samples from a Gamma distribution to a mixture of gaussians.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Generate training data","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"if !@isdefined(data)\n    T = 100\n    gamma_k = 8 # shape\n    gamma_θ = 1 # scale\n    gamma_mode = gamma_k > 1 ? (gamma_k - 1) * gamma_θ : 0\n    gamma_mean = gamma_k * gamma_θ\n    gamma_std = sqrt(gamma_k) * gamma_θ\n    data_disn = Expr(:call, :Gamma, gamma_k, gamma_θ)\n    data_dis = eval(data_disn)\n    data_score = derivative(logpdf(data_dis))\n    data = Float32.(rand(data_dis, T))\n    xlims = (-1, 25)\n    xticks = [0, floor(Int, minimum(data)), gamma_mode, gamma_mean, ceil(Int, maximum(data))]\n    xticks = sort(xticks) # ticks that span the data range\n\n    pfd = scatter(data, zeros(T); xlims, xticks, color=:black)\n    plot!(pfd, pdf(data_dis); label=\"$data_disn pdf\",\n        color = :black, xlabel = L\"x\", ylabel = L\"p(x)\")\n\n    psd = plot(data_score; color=:black,\n        label = \"$(data_disn.args[1]) score function\",\n        xaxis=(L\"x\", (1,20), xticks), ylims = (-3, 5), yticks=[0,4],\n    )\n    psdn = deepcopy(psd)\n    tmp = score(Normal(mean(data), std(data)))\n    plot!(psdn, tmp; label = \"Normal score function\", line=:dash, color=:black)\n\n    ph = histogram(data; linecolor=:blue,\n        xlabel=L\"x\", size=(600,300), yaxis=(\"count\", (0,15), 0:5:15),\n        bins=-1:0.5:25, xlims, xticks, label=\"data histogram\")\n    # Plots.savefig(ph, \"gamma-data.pdf\")\n    plot!(ph, x -> T*0.5 * pdf(data_dis)(x);\n        color=:black, label=\"$data_disn Distribution\")\n    # Plots.savefig(ph, \"gamma-fit.pdf\")\nend\n\n\nif false # plots for a talk\n    pdt = plot(pdf(data_dis); label=\"$data_disn pdf\", color = :blue,\n        left_margin, bottom_margin,\n        xaxis = (L\"x\", xlims, xticks), ylabel = L\"p(x)\", size=(600,200))\n    pst = deepcopy(psd)\n    tmp = score(Normal(gamma_mean, gamma_std))\n    plot!(pst, tmp; label = \"Normal score function\", size=(600,200), xlims,\n        line=:dash, color=:magenta, left_margin, bottom_margin,\n        ylabel=ylabel_score1)\n\n    # Plots.savefig(pdt, \"gamma-pdf.pdf\")\n    # Plots.savefig(pst, \"gamma-score.pdf\")\nend","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"To perform unconstrained minimization of a D-component mixture, the following mapping from mathbbR^D-1 to the D-dimensional simplex is helpful. It is the inverse of the additive logratio transform. It is related to the softmax function.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"function map_r_s(y::AbstractVector; scale::Real = 1.0)\n    y = scale * [y; 0]\n    y .-= maximum(y) # for numerical stability\n    p = exp.(y)\n    return p / sum(p)\nend\nmap_r_s(y::Real...) = map_r_s([y...])\n\ny1 = range(-1,1,101) * 9\ny2 = range(-1,1,101) * 9\ntmp = map_r_s.(y1, y2')\npj = jim(y1, y2, tmp; title=\"Simplex parameterization\", nrow=1)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Define model distribution","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"nmix = 3 # how many gaussians in the mixture model\nfunction model(θ ;\n    σmin::Real = 1,\n    σmax::Real = 19,\n)\n    mu = θ[1:nmix]\n    sig = θ[nmix .+ (1:nmix)]\n    any(<(σmin), sig) && throw(\"too small σ\")\n    any(>(σmax), sig) && throw(\"too big σ $sig\")\n    # sig = σmin .+ exp.(sig) # ensure σ > 0\n    # sig = @. σmin + (σmax - σmin) * (tanh(sig/2) + 1) / 2 # \"constraints\"\n    p = map_r_s(θ[2nmix .+ (1:(nmix-1))])\n    tmp = [(μ,σ) for (μ,σ) in zip(mu, sig)]\n    mix = MixtureModel(Normal, tmp, p)\n    return mix\nend;\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Define explicit score-matching cost function","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"function cost_esm2(x::AbstractVector{<:Real}, θ)\n    model_score = score(model(θ))\n    return (0.5/T) * sum(abs2, model_score.(x) - data_score.(x))\nend;\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Minimize this explicit score-matching cost function:","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"β = 0e-4 # optional small regularizer to ensure coercive\ncost_esm1 = (θ) -> cost_esm2(data, θ) + β * 0.5 * norm(θ)^2;\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Initial crude guess of mixture model parameters","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"θ0 = Float64[5, 7, 9, 1.5, 1.5, 1.5, 0, 0]; # Gamma\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Plot data pdf and initial model pdf","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"pf = deepcopy(pfd)\nplot!(pf, pdf(model(θ0)), label = \"Initial Gaussian mixture\", color=:blue)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Check descent and non-convexity","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"if false\n    tmp = gradient(cost_esm1)(θ0)\n    a = range(0, 9, 101)\n    h = a -> cost_esm1(θ0 - a * tmp)\n    plot(a, log.(h.(a)))\nend","category":"page"},{"location":"generated/examples/01-overview/#Explicit-score-matching-(ESM)-(impractical)","page":"Score Matching overview","title":"Explicit score matching (ESM) (impractical)","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"if !@isdefined(θesm)\n    lower = [fill(0, nmix); fill(1.0, nmix); fill(-Inf, nmix-1)]\n    upper = [fill(Inf, nmix); fill(Inf, nmix); fill(Inf, nmix-1)]\n    opt_esm = optimize(cost_esm1, lower, upper, θ0, Fminbox(BFGS());\n        autodiff = :forward)\n    # opt_esm = optimize(cost_esm1, θ0, BFGS(); autodiff = :forward) # unconstrained\n    θesm = minimizer(opt_esm)\nend;\n\nplot!(pf, pdf(model(θesm)), label = \"ESM Gaussian mixture\", color=:green)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Plot the data score and model score functions to see how well they match. The largest mismatch is in the tails of the distribution where there are few (if any) data points.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"ps = deepcopy(psd)\nplot!(ps, score(model(θesm)); label = \"ESM score function\", color=:green)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/#Maximum-likelihood-estimation","page":"Score Matching overview","title":"Maximum-likelihood estimation","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"This toy example is simple enough that we can apply ML estimation to it directly. In fact, ML estimation is a seemingly more practical optimization problem than score matching in this case.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"As expected, ML estimation leads to a lower negative log-likelihood.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"negloglike(θ) = (-1/T) * sum(logpdf(model(θ)), data)\nopt_ml = optimize(negloglike, lower, upper, θ0, Fminbox(BFGS()); autodiff = :forward)\nθml = minimizer(opt_ml)\nnegloglike.([θml, θesm, θ0])","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Curiously, ML estimation here leads to much worse fits to the pdf than score matching, even though we initialized the ML optimizer with the score-matching parameters. Perhaps the landscape of the log-likelihood is less well-behaved than that of the SM cost.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"plot!(pf, pdf(model(θml)), label = \"ML Gaussian mixture\", color=:magenta)\nplot!(ps, score(model(θml)), label = \"ML score function\", color=:magenta)\nplot(pf, ps)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/#Implicit-score-matching-(ISM)-(more-practical)","page":"Score Matching overview","title":"Implicit score matching (ISM) (more practical)","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"The above ESM fitting process used score(data_dis), the score-function of the data distribution, which is unknown in practical situations.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Hyvärinen 2005 derived the following more practical cost function that is independent of the unknown data score function:","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"J_mathrmISM(bmθ) =\nfrac1T _t=1^T\n_i=1^N _i s_i(bmx_t bmθ)\n + frac12  s_i(bmx_t bmθ) ^2","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"ignoring a constant that is independent of θ where","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"_i s_i(bmx bmθ)\n=\nfrac x_i s_i(bmx bmθ)\n=\nfrac^2 x_i^2 log p(bmx bmθ)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"(For large models this version is still a bit impractical because it depends on the diagonal elements of the Hessian of the log prior. Subsequent pages deal with that issue.)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Vincent, 2011 calls this approach implicit score matching (ISM).","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Implicit score-matching cost function","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"function cost_ism2(x::AbstractVector{<:Real}, θ)\n    tmp = model(θ)\n    model_score = score(tmp)\n    return (1/T) * (sum(score_deriv(tmp), x) +\n        0.5 * sum(abs2 ∘ model_score, x))\nend;\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Minimize this implicit score-matching cost function:","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"if !@isdefined(θism)\n    cost_ism1 = (θ) -> cost_ism2(data, θ)\n    opt_ism = optimize(cost_ism1, lower, upper, θ0, Fminbox(BFGS()); autodiff = :forward)\n    ##opt_ism = optimize(cost_ism1, θ0, BFGS(); autodiff = :forward)\n    θism = minimizer(opt_ism)\n    cost_ism1.([θism, θesm, θml])\nend;\n\nplot!(pf, pdf(model(θism)), label = \"ISM Gaussian mixture\", color=:cyan)\nplot!(ps, score(model(θism)), label = \"ISM score function\", color=:cyan)\npfs = plot(pf, ps)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Curiously the supposedly equivalent ISM cost function works much worse. Like the ML estimate, the first two σ values are stuck at the lower limit. Could it be local extrema? More investigation is needed!","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Ideally (as T  ), the ESM and ISM cost functions should differ by a constant independent of θ. Here they differ for small, finite T.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"tmp = [θ0, θesm, θml, θism]\ncost_esm1.(tmp) - cost_ism1.(tmp)","category":"page"},{"location":"generated/examples/01-overview/#Regularized-score-matching-(RSM)","page":"Score Matching overview","title":"Regularized score matching (RSM)","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Kingma & LeCun, 2010 reported some instability of ISM and suggested a regularized version corresponding to the following (practical) cost function:","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"J_mathrmRSM(bmθ) =\nJ_mathrmISM(bmθ) + λ R(bmθ)\nquad\nR(bmθ) =\nfrac1T _t=1^T\n_i=1^N  _i s_i(bmx_t bmθ) ^2","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Regularized score matching (RSM) cost function","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"function cost_rsm2(x::AbstractVector{<:Real}, θ, λ)\n    mod = model(θ)\n    model_score = score(mod)\n    tmp = score_deriv(mod).(x)\n    R = sum(abs2, tmp)\n    J_ism = sum(tmp) + 0.5 * sum(abs2 ∘ model_score, x)\n    return (1/T) * (J_ism + λ * R)\nend;\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Minimize this RSM cost function:","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"λ = 4e-1\ncost_rsm1 = (θ) -> cost_rsm2(data, θ, λ)\n\nif !@isdefined(θrsm)\n    opt_rsm = optimize(cost_rsm1, lower, upper, θ0, Fminbox(BFGS());\n        autodiff = :forward)\n    θrsm = minimizer(opt_rsm)\n    cost_rsm1.([θrsm, θ0, θism, θesm, θml])\nend","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"plot!(pf, pdf(model(θism)), label = \"RSM Gaussian mixture\", color=:red)\nplot!(ps, score(model(θism)), label = \"RSM score function\", color=:red)\npfs = plot(pf, ps)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Sadly the regularized score matching (RSM) approach did not help much here. Increasing λ led to optimize errors.","category":"page"},{"location":"generated/examples/01-overview/#Denoising-score-matching-(DSM)","page":"Score Matching overview","title":"Denoising score matching (DSM)","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Vincent, 2011 proposed a practical approach called denoising score matching (DSM) that matches the model score function to the score function of a Parzen density estimate of the form","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"q_σ(bmx) = frac1T _t=1^T g_σ(bmx - bmx_t)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"where g_σ denotes a Gaussian distribution mathcalN(bm0 σ bmI).","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Statistically, this approach is equivalent (in expectation) to adding noise to the measurements, and then applying the ESM approach. The DSM cost function is","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"J_mathrmDSM(bmθ) =\nfrac1T _t=1^T\nmathbbE_bmz  g_σleft\nfrac12\nleft\nbms(bmx_t + bmz bmθ) + fracbmzσ^2\nright_2^2\nright","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"A benefit of this approach is that it does not require differentiating the model score function w.r.t bmx.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"The inner expectation over g_σ is typically analytically intractable, so in practice we replace it with a sample mean where we draw M  1 values of bmz per training sample, leading to the following practical cost function","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"J_mathrmDSM  M(bmθ) =\nfrac1T _t=1^T\nfrac1M _m=1^M\nfrac12\nleft\ns(bmx_t + bmz_tm bmθ) + fracbmz_tmσ^2\nright_2^2","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"where the noise samples bmz_tm are IID.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"The next code blocks investigate this DSM approach for somewhat arbitrary choices of M and σ.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"seed!(0)\nM = 9\nσdsm = 1.0\nzdsm = σdsm * randn(T, M);\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Define denoising score-matching cost function, where input data has size (T) and input z has size (TM).","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"function cost_dsm2(data::AbstractVector{<:Real}, z::AbstractArray{<:Real}, θ)\n    model_score = score(model(θ))\n    tmp = model_score.(data .+ z) # (T,M) # add noise to data\n    return (0.5/T/M) * sum(abs2, tmp + z ./ σdsm^2)\nend;\n\nif !@isdefined(θdsm)\n    cost_dsm1 = (θ) -> cost_dsm2(data, zdsm, θ) # + β * 0.5 * norm(θ)^2;\n    opt_dsm = optimize(cost_dsm1, lower, upper, θ0, Fminbox(BFGS());\n        autodiff = :forward)\n    θdsm = minimizer(opt_dsm)\nend;\n\nplot!(pf, pdf(model(θdsm)); label = \"DSM Gaussian mixture\", color=:orange)\nplot!(ps, score(model(θdsm)); label = \"DSM score function\", color=:orange)\npfs = plot(pf, ps)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"At least for this case, DSM worked better than ML, ISM and RSM.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/#Noise-conditional-score-matching-(NCSM)","page":"Score Matching overview","title":"Noise-conditional score-matching (NCSM)","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Above we used a single noise value for DSM. Contemporary methods use a range of noise values with noise-conditional score models, e.g., Song et al. ICLR 2021.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"A representative formulation uses a σ^2-weighted expectation like the following:","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"J_mathrmNCSM(bmθ) \nmathbbE_σ  f(σ)left σ^2 J_mathrmDSM(bmθ σ) right","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"J_mathrmDSM(bmθ σ) \nfrac1T _t=1^T\nmathbbE_bmz  g_σleft\nfrac12\nleft\nbms(bmx_t + bmz bmθ σ) + fracbmzσ^2\nright_2^2\nright","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"for some distribution f(σ) of noise levels.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Here we use a multi-layer perceptron (MLP) to model the 1D noise-conditional score function bms(bmx bmθ σ). We use a residual approach with data normalization and the baseline score of a standard normal distribution.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"function make_nnmodel(\n    data; # (2,?)\n    nweight = [2, 8, 16, 8, 1], # 2 inputs: [x, σ] for NCSM\n    μdata = mean(data),\n    σdata = std(data),\n)\n    layers = [Dense(nweight[i-1], nweight[i], Flux.gelu) for i in 2:length(nweight)]\n    # Change of variables y ≜ (x - μdata) / σ(x + z)\n    pick1(x) = transpose(x[1,:]) # extract 1D data as a row vector\n    pick2(x) = transpose(x[2,:]) # extract σ as a row vector\n    quad(σ) = sqrt(σ^2 + σdata^2) # σ(x + z) (quadrature)\n    scale1(x) = [(pick1(x) .- μdata) ./ quad.(pick2(x)); pick2(x)] # standardize\n    # The baseline score function here is just -y; use in a \"ResNet\" way:\n    tmp1 = Flux.Parallel(.-, Chain(layers...), pick1)\n    scale2(σ) = quad.(σ) / σdata^2 # \"un-standardize\" for final score w.r.t x\n    tmp2 = Flux.Parallel(.*, tmp1, scale2 ∘ pick2)\n    return Chain(scale1, tmp2)\nend;\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Function to make data pairs suitable for NN training. Each use of this function's output is akin to M epochs of data. Use shuffle in case we use mini-batches later.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"function dsm_data(\n    M::Int = 9,\n    σdist = Uniform(0.2,2.0),\n)\n    σdsm = Float32.(rand(σdist, T, M))\n    z = σdsm .* randn(Float32, T, M)\n    tmp1 = shuffle(data) .+ z # (T,M)\n    tmp2 = transpose([vec(tmp1) vec(σdsm)]) # (2, T*M)\n    return (tmp2, -transpose(vec(z ./ σdsm.^2)))\nend\n\nif !@isdefined(nnmodel)\n    nnmodel = make_nnmodel(data;)\n\n    # σ^2-weighted MSE loss:\n    loss3(model, x, y) = mean(abs2, (model(x) - y) .* transpose(x[2,:])) / 2\n\n    iters = 2^9\n    dataset = [dsm_data() for i in 1:iters]\n    state1 = Flux.setup(Adam(), nnmodel)\n    @info \"begin train\"\n    @time Flux.train!(loss3, nnmodel, dataset, state1)\n    @info \"end train\"\nend\n\nnnscore = x -> nnmodel([x, 0.4])[1] # lower end of σdist range\ntmp = deepcopy(ps)\nplot!(tmp, nnscore; label = \"NN score function\", color=:blue)","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"The noise-conditional NN score model worked OK. Training \"coarse to fine\" might work better than the Uniform approach above.","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"prompt()\n\nif false # look at the set of NN score functions\n    tmp = deepcopy(psd)\n    for s in 0.1:0.2:1.6\n        plot!(tmp, x -> nnmodel([x, s])[1]; label = \"NN score function $s\")\n    end\n    gui()\nend","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"Plots of data distribution with various added noise levels","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"σlist = [0.0005, 0.05, 0.1, 0.5, 1, 5]\nnσ = length(σlist)\npl = Array{Any}(undef, nσ)\nfor (i, σ) in enumerate(σlist)\n    local mix = MixtureModel(Normal, [(d,σ) for d in data])\n    xm = range(0, 20, step = min(σ/5, 0.1))\n    local tmp = pdf(mix).(xm)\n    pl[i] = plot(xm, tmp; color = :red,\n        xaxis = (L\"x\", (0, 20), [0, gamma_mean, 18]),\n        yaxis = (L\"q_σ(x)\", [0,]), title = \"σ = $σ\",\n    )\n    # plot!(pl, xm, tmp / maximum(tmp), label = \"σ = $σ\")\nend\nplot(pl...)\n\n# Plots.savefig(\"data-qsig.pdf\")\nprompt()","category":"page"},{"location":"generated/examples/01-overview/#Reproducibility","page":"Score Matching overview","title":"Reproducibility","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"This page was generated with the following version of Julia:","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"using InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"And with the following package versions","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"import Pkg; Pkg.status()","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"","category":"page"},{"location":"generated/examples/01-overview/","page":"Score Matching overview","title":"Score Matching overview","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = ScoreMatching","category":"page"},{"location":"#ScoreMatching.jl-Documentation","page":"Home","title":"ScoreMatching.jl Documentation","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This Julia package ScoreMatching provides tools for fitting statistical distributions to training data.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Here, \"score\" means the gradient of a log-likelihood nabla_mathbfx log p(mathbfx mathbftheta) as defined  by Hyvärinen 2005.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See the \"Examples\" for details.","category":"page"}]
}
