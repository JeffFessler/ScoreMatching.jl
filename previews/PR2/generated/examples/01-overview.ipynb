{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Score Matching overview\n",
    "\n",
    "This page introduces the Julia package\n",
    "[`ScoreMatching`](https://github.com/JeffFessler/ScoreMatching.jl).\n",
    "\n",
    "This page was generated from a single Julia file:\n",
    "[01-overview.jl](https://github.com/JeffFessler/ScoreMatching.jl/blob/main/docs/lit/examples/01-overview.jl)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Packages needed here."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ScoreMatching\n",
    "using MIRTjim: jim, prompt\n",
    "using Distributions: Distribution, Gamma, Normal, MixtureModel, logpdf, pdf\n",
    "import Distributions: logpdf, pdf\n",
    "import ForwardDiff\n",
    "using LinearAlgebra: tr\n",
    "using LaTeXStrings\n",
    "using Random: seed!; seed!(0)\n",
    "using Optim: optimize, BFGS, Fminbox\n",
    "import Optim: minimizer\n",
    "using Plots: plot, plot!, scatter, default, gui\n",
    "using InteractiveUtils: versioninfo\n",
    "default(label=\"\", markerstrokecolor=:auto)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following line is helpful when running this file as a script;\n",
    "this way it will prompt user to hit a key after each figure is displayed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isinteractive() ? jim(:prompt, true) : prompt(:draw);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "Given $T$\n",
    "[IID](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "training data samples\n",
    "$\\mathbf{x}_1, …, \\mathbf{x}_T ∈ \\mathbb{R}^N$,\n",
    "we often want to find the parameters\n",
    "$\\mathbf{θ}$\n",
    "of a model distribution\n",
    "$p(\\mathbf{x}; \\mathbf{θ})$\n",
    "that \"best fit\" the data.\n",
    "\n",
    "Maximum-likelihood estimation\n",
    "is impractical for complicated models\n",
    "where the normalizing constant is intractable.\n",
    "\n",
    "[Hyvärinen 2005](http://jmlr.org/papers/v6/hyvarinen05a.html)\n",
    "proposed an alternative called\n",
    "_score matching_\n",
    "that circumvents\n",
    "needing to find the normalizing constant.\n",
    "\n",
    "The idea behind the score matching approach\n",
    "to model fitting is\n",
    "$$\n",
    "\\hat{\\mathbf{θ}} = \\arg \\min_{\\mathbf{θ}}\n",
    "J(\\mathbf{θ})\n",
    ",\\qquad\n",
    "J(\\mathbf{θ}) =\n",
    "\\frac{1}{T} ∑_{t=1}^T\n",
    "\\| \\mathbf{s}(\\mathbf{x}_t; \\mathbf{θ}) - \\mathbf{s}(\\mathbf{x}_t) \\|^2\n",
    "$$\n",
    "where\n",
    "$\n",
    "\\mathbf{s}(\\mathbf{x}; \\mathbf{θ}) =\n",
    "\\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}; \\mathbf{θ})\n",
    "$\n",
    "is the _score function_\n",
    "of the model distribution,\n",
    "and\n",
    "$\n",
    "\\mathbf{s}(\\mathbf{x}) =\n",
    "\\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})\n",
    "$\n",
    "is the _score function_\n",
    "of the (typically unknown) data distribution.\n",
    "\n",
    "\n",
    "## Illustration\n",
    "\n",
    "For didactic purposes,\n",
    "we illustrate this basic version of score matching\n",
    "by fitting samples from a\n",
    "[Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution)\n",
    "to a mixture of gaussians."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some convenience methods"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "logpdf(d::Distribution) = x -> logpdf(d, x)\n",
    "pdf(d::Distribution) = x -> pdf(d, x)\n",
    "derivative(f::Function) = x -> ForwardDiff.derivative(f, x)\n",
    "gradient(f::Function) = x -> ForwardDiff.gradient(f, x)\n",
    "score(d::Distribution) = derivative(logpdf(d))\n",
    "score_deriv(d::Distribution) = derivative(score(d)) # scalar x only"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate training data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "T = 100\n",
    "data_dis = Gamma(8, 1.0)\n",
    "data_score = derivative(logpdf(data_dis))\n",
    "data = rand(data_dis, T)\n",
    "pd = scatter(data, zeros(T))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform unconstrained minimization\n",
    "of a $D$-component mixture,\n",
    "the following mapping from $\\mathbb{R}^{D-1}$\n",
    "to the $D$-dimensional simplex is helpful.\n",
    "It is the inverse of the\n",
    "[additive logratio transform](https://en.wikipedia.org/wiki/Compositional_data#Additive_logratio_transform)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function map_r_s(y::AbstractVector)\n",
    "    p = exp.([y; 0])\n",
    "    return p / sum(p)\n",
    "end\n",
    "map_r_s(y::Real...) = map_r_s([y...])\n",
    "\n",
    "y1 = range(-1,1,101) * 9\n",
    "y2 = range(-1,1,101) * 9\n",
    "tmp = map_r_s.(y1, y2')\n",
    "jim(y1, y2, tmp; title=\"Simplex parameterization\", nrow=1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define model distribution"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nmix = 3 # how many gaussians in the mixture model\n",
    "function model(θ ; σmin::Real=1e-2)\n",
    "    mu = θ[1:nmix]\n",
    "    sig = θ[nmix .+ (1:nmix)]\n",
    "    any(<(σmin), sig) && throw(\"bad σ\")\n",
    "    # sig = σmin .+ exp.(sig) # ensure σ > 0\n",
    "    p = map_r_s(θ[2nmix .+ (1:(nmix-1))])\n",
    "    tmp = [(μ,σ) for (μ,σ) in zip(mu, sig)]\n",
    "    return MixtureModel(Normal, tmp, p)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define score-matching cost function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function cost_sm2(x::AbstractVector{<:Real}, θ)\n",
    "    model_score = score(model(θ))\n",
    "    return sum(abs2, model_score.(x) - data_score.(x)) / T\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Minimize this score-matching cost function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cost_sm1 = (θ) -> cost_sm2(data, θ);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initial crude guess of mixture model parameters"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "θ0 = [5, 7, 9, 1.5, 1.5, 1.5, 0, 0];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot data pdf and initial model pdf"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pf = plot(pdf(data_dis); xlims = (-1, 25), label=\"Gamma pdf\",\n",
    "    color = :black,\n",
    "    xlabel = L\"x\",\n",
    "    ylabel = L\"p(x) \\ \\mathrm{ and } \\ p(x;θ)\",\n",
    ")\n",
    "plot!(pf, pdf(model(θ0)), label = \"Initial Gaussian mixture\", color=:blue)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Impractical score matching"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lower = [fill(0, nmix); fill(1.0, nmix); fill(-Inf, nmix-1)]\n",
    "upper = [fill(Inf, nmix); fill(Inf, nmix); fill(Inf, nmix-1)]\n",
    "opt_sm = optimize(cost_sm1, lower, upper, θ0, Fminbox(BFGS()); autodiff = :forward)\n",
    "θsm = minimizer(opt_sm)\n",
    "\n",
    "plot!(pf, pdf(model(θsm)), label = \"SM Gaussian mixture\", color=:red)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the data score and model score functions\n",
    "to see how well they match.\n",
    "The largest mismatch is in the tails of the distribution\n",
    "where there are few (if any) data points."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ps = plot(data_score; xlims=(1,20), label = \"Data score function\",\n",
    "    xticks=[1,20], xlabel=L\"x\", color=:black)\n",
    "plot!(ps, score(model(θsm)); label = \"SM score function\", color=:red)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Maximum-likelihood estimation\n",
    "\n",
    "This toy example is simple enough\n",
    "that we can apply ML estimation to it directly.\n",
    "In fact, ML estimation is a seemingly more practical optimization problem\n",
    "than score matching in this case.\n",
    "\n",
    "As expected,\n",
    "ML estimation leads to a lower negative log-likelihood."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "negloglike(θ) = (-1/T) * sum(logpdf(model(θ)), data)\n",
    "opt_ml = optimize(negloglike, lower, upper, θsm, Fminbox(BFGS()); autodiff = :forward)\n",
    "θml = minimizer(opt_ml)\n",
    "negloglike.([θml, θsm, θ0])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Curiously,\n",
    "ML estimation here leads to much worse fits to the pdf\n",
    "than score matching,\n",
    "even though we initialized the ML optimizer\n",
    "with the score-matching parameters.\n",
    "Perhaps the landscape of the log-likelihood\n",
    "is less well-behaved\n",
    "than that of the SM cost."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot!(pf, pdf(model(θml)), label = \"ML Gaussian mixture\", color=:magenta)\n",
    "plot!(ps, score(model(θml)), label = \"ML score function\", color=:magenta)\n",
    "plot(pf, ps)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Practical score matching\n",
    "\n",
    "The above SM fitting process\n",
    "used `score(data_dis)`,\n",
    "the score-function of the data distribution,\n",
    "which is unknown in practical situations.\n",
    "\n",
    "[Hyvärinen 2005](http://jmlr.org/papers/v6/hyvarinen05a.html)\n",
    "derived the following more practical cost function\n",
    "that is independent of the unknown data score function:\n",
    "$$\n",
    "J(\\mathbf{θ}) =\n",
    "\\frac{1}{T} ∑_{t=1}^T\n",
    "∑_{i=1}^N ∂_i s_i(\\mathbf{x}_t; \\mathbf{θ})\n",
    " + \\frac{1}{2} | s_i(\\mathbf{x}_t; \\mathbf{θ}) |^2,\n",
    "$$\n",
    "ignoring a constant that is independent of $θ,$\n",
    "where\n",
    "$$\n",
    "∂_i s_i(\\mathbf{x}; \\mathbf{θ})\n",
    "=\n",
    "\\frac{∂}{∂ x_i} s_i(\\mathbf{x}; \\mathbf{θ})\n",
    "=\n",
    "\\frac{∂^2}{∂ x_i^2} \\log p(\\mathbf{x}; \\mathbf{θ}).\n",
    "$$\n",
    "\n",
    "(For large models\n",
    "this version is still a bit impractical\n",
    "because it depends on the diagonal\n",
    "elements of the Hessian\n",
    "of the log prior.\n",
    "Subsequent pages deal with that issue.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Practical score-matching cost function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function cost_sp2(x::AbstractVector{<:Real}, θ)\n",
    "    tmp = model(θ)\n",
    "    model_score = score(tmp)\n",
    "    return (1/T) * (sum(score_deriv(tmp), x) +\n",
    "        0.5 * sum(abs2 ∘ model_score, x))\n",
    "end;\n",
    "\n",
    "cost_sp1 = (θ) -> cost_sp2(data, θ) # minimize this score-matching cost function\n",
    "opt_sp = optimize(cost_sp1, lower, upper, θ0, Fminbox(BFGS()); autodiff = :forward)\n",
    "θsp = minimizer(opt_sp)\n",
    "cost_sp1.([θsp, θsm, θml])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot!(pf, pdf(model(θsp)), label = \"SP Gaussian mixture\", color=:cyan)\n",
    "plot!(ps, score(model(θsp)), label = \"SP score function\", color=:cyan)\n",
    "pfs = plot(pf, ps)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Curiously the supposedly equivalent SM cost function works much worse.\n",
    "Like the ML estimate,\n",
    "the first two $σ$ values are stuck at the `lower` limit.\n",
    "Could it be local extrema?\n",
    "More investigation is needed!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reproducibility"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This page was generated with the following version of Julia:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "io = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And with the following package versions"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Pkg; Pkg.status()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "kernelspec": {
   "name": "julia-1.8",
   "display_name": "Julia 1.8.5",
   "language": "julia"
  }
 },
 "nbformat": 4
}
